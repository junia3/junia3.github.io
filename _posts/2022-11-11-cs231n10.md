---
title: cs231n 내용 요약 (10) - Visualizing, Transfer learning
layout: post
description: Lecture summary
use_math: true
post-image: https://user-images.githubusercontent.com/79881119/210934049-762c2540-c097-4107-89ba-32bd2b8bc9f3.png
category: deep learning
tags:
- AI
- Deep learning
- cs231n
---

# 들어가며...

이 글이 아마도 cs231n과 관련된 <U>마지막 포스팅</U>이 될 것이다. 사실 깃허브 블로그를 오픈하고 기존 네이버 블로그에 작성했던 내용들을 다시 원래 강의 노트와 비교하면서 옮기고 있었는데, 예전에 작성했던 내용들을 보니 애매하게 적어둔 내용도 많고 잘 모르고 작성한 부분들도 은근 많았던 것 같다. 1년 전이기 때문에 지금도 그때와 비교해서 더 많이 아는 건 아니지만 정리하면서 최대한 예전에 공부했던 내용들을 다시 살펴보는 것이 <U>기초를 다지는 과정</U>에 효과적인 것 같다.   
지금부터 다룰 내용은 이전에 신경망의 구조나 parameter, functionality에 대해 살펴본 것보다 수식적으로 증명하거나 풀어낼 부분이 많지는 않다.

---

# Visualizing what convolutional neural networks learn
CNN이 처음으로 <U>ImageNet 대회에서 우승</U>한 이후로 deep learning, 그것도 neural network 기반의 알고리즘이 주목을 받기 시작했다. 

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219004015-6a3aba22-1fc1-4853-80a9-c30f9d39e28a.png" width="500">
</p>

표에서 보이는 XRCE라는 알고리즘은 딥러닝 기반이 아니었으며, 2011년 이전의 error가 표시되어있지는 않으나 사실상 2011년의 error rate가 딥러닝을 사용하지 않은 방식을 통해서는 얻을 수 있는 <U>최소의 수렴값으로 인식</U>되기 시작했다. 바로 이러한 인식과 알고리즘의 판도를 바꾼 game changer로 등장한 것이 AlexNet이었고, 무려 $10\%$의 성능 향상을 보이며 이후 대회에서는 모두 <U>deep learning network가 우승</U>하기 시작했다.   
하지만 딥러닝 기반의 CNN이 ImageNet에서 우승한 이후에도 지속적인 의문점과 비판이 올라오기 시작했다. 대부분 예전 low-level vision task를 기계 학습과 관련된 여러 컴퓨터 알고리즘으로 해결하고자 했던 사람들이었고, 비판하는 내용은 대부분 deep learning 알고리즘은 연구의 판도를 바꿀 정도로 성능 향상에 큰 기여를 했으나 <U>작동 원리에 대한 엄밀한 설명이 불가능</U>하다는 주장이었다.
이전 포스팅까지 소개했었던 <U>MLP(Multi-Layer Perceptron)</U>과 같은 구조를 계속 설명했었고 네트워크가 학습되는 과정을 forward propagation, objective function 그리고 back-propagation의 순서대로 개념을 설명했었다. 결국 loss function을 최적화하는 방향으로 학습하는 과정을 이해하기 위해서는 gradient, linear algebra 등등 수학적인 지식이 필요하지만, 딥러닝이라는 분야가 <U>하나의 연구 분야로 인정받기 위해서</U>는 구체적으로 네트워크가 왜 이런 방식으로 학습을 하는 것이 <U>기적적인 성능 향상</U>을 이끌어냈는지 수학적 설명이 뒷받침될 필요가 있었다.   
Computer science에서는 이러한 의문이 중요한 문제로 자리잡았다. Data-driven 알고리즘은 deterministic 알고리즘과는 다르게 원리를 파악하고 개선시키고자 하는 방향을 잡을 수 없기 때문이다. 단순히 인간의 neuron 구조를 모방한 perceptron이 MLP로, 더 나아가 computer vision task를 위한 CNN으로 발전되었고 좋은 성능을 보인다는 점에서 <U>설명력이 부족하다는</U> 비판이 올라오게 되었다.

이렇듯 Neural network 구조의 input과 output에 대한 수학적 설명은 가능하지만, 내부 parameter가 prediction에 generalized될 수 있는 근거를 설명할 수 없다는 사실을 <U>'black box'</U>라는 용어를 통해 표현하게 되었다. 단순히 neural network에서는 이러한 black box(explicit한 input, output을 제외하고는 implicit하게 학습되는 내부를 직접 관측할 수 없어 원리를 설명할 수 없음)의 문제점을 해결하지 못했다. 처음 ImageNet 대회에서 우승했던 AlexNet도 논문화 과정에서 <U>해당 내용들을 자세히 서술하지 못해</U> 문제가 되었다.

그리고 단순히 학계에 있는 사람들을 설득해야할 필요성 뿐만 아니라 네트워크의 성능에 대한 설명이 필요했던 이유는 neural network 설계 과정에서 성능을 높이거나 error가 발생한 sample에 대해 explainablilty가 있어야 추후 딥러닝 연구가 가능했기 때문이다. 결국 deep learning에서 explainablilty는 기존 학계에 있던 <U>사람들을 설득하는 과정</U>에서도, 본인들의 <U>연구를 발전시키기 위한 기반</U>으로도 필요했던 부분인 것이다.

그렇기 때문에 deep learning network 내부에서 학습하는 형태를 간접적으로 확인하고자 weight visualization 혹은 특정 input $x$에 대한 layer activation $(f_l \circ f_{l-1} \circ \cdots \circ f_1(x))$ visualization과 같이 성능에 대한 <U>설명력을 뒷받침할 연구</U>들이 진행되었다.

---

# Layer activations
Visualizing 기술 중 하나는 forward pass에서 input에 대한 layer의 activation 결과를 보여주는 것이다. ReLU가 달려있는 네트워크에서는 activation을 관찰하게 되면 초반에는 blobby하고 dense한 모습을 보여주지만 training이 진행되면 될수록 sparse하고 localized된 모습을 보여준다.

...작성중