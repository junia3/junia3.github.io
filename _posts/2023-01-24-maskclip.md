---
title: MaskCLIP 논문 리뷰
layout: post
description: Zero-shot, Multimodal learning
use_math: true
post-image: https://user-images.githubusercontent.com/79881119/214473940-413c366a-cf7d-4431-883e-64ae6097be86.png
category: paper review
tags:
- Multimodal
- Zero shot
- SSL
- AI
- Deep learning
---

# 들어가며...

사실 리뷰할 MaskCLIP은 ECCV paper와 arxiv paper 두 종류가 있다. 논문 제목은 다르지만 main idea의 제목이 같다보니 조금 혼란스러운 감이 없지 않아 있었다. 처음에 읽고 싶었던 논문은 '[Extract Free Dense Labels from CLIP](https://arxiv.org/abs/2112.01071)'이며, 그 다음에 추가로 arxiv에 올라온 논문 제목은 '[MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining](https://arxiv.org/abs/2208.12262)'으로, 아직 학회에 제출을 하진 않은 것 같지만 느낌상 NeurIPS 2023에 올라올 것 같은 내용이다. ECCV 2022에 올라갔던 논문인 Dense label extraction은 segmentation task에 댇해 CLIP의 zero-shot representation을 어떻게 하면 가장 잘 활용할 수 있을까에 대한 실험을 진행했던 연구였고, 아카이브에 올라온 논문인 self-distillation은 CLIP의 image representation 학습에 self-supervised learning 방식 중 하나인 generative approach(masked autoencoder 방식)을 적용한 연구다. 

---

# CLIP의 장점?
VL(Visual-language) contrastive learning를 통해 vision의 modality인 image와 text modality와의 관계성을 찾은 것이 CLIP 논문이었고, 해당 논문을 기반으로 다양한 dataset의 downstream task(classification, object detection 그리고 segmentation 등등)을 zero-shot 혹은 few-shot 으로 해결할 수 있는 길이 열리게 되었다.
<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214461848-424e4ad3-6f46-4ccf-b7e2-15559865bc6b.png" width="500"/>
</p>
사실상 GPT와 같은 대용량 언어 모델 이외에 computer vision에서 웹 상에서 획득할 수 있는 text prompt 기반 대용량 데이터셋을 학습에 사용한 것은 CLIP이 처음이었으며, 단순히 downstream task를 잘 해결할 뿐만 아니라 language와 vision의 관계를 찾은 것에서 image captioning, view synthesis와 같이 prompt 기반 DL engineering이 가능해졌다는 점에서 <U>멀티모달의 새 시대를 연 장본인</U>이라고 볼 수 있다.

---

# Segmentation에서는 활용할 수 없을까?
ECCV 2022 논문인 'Extract Free Dense Labels from CLIP'는 바로 이러한 CLIP의 장점에 집중하여, 어떻게 하면 CLIP의 zero-shot performance를 segmentation에서도 활용할 수 있을까에 대해 연구를 진행하였다.
<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214462980-32bef7a2-dce7-4c90-bba2-3d18798696f9.png" width="500"/>
</p>
이는 굉장히 큰 의미가 있었던 것이 만약 우리가 위와 같은 그림을 segmentation해야 한다고 가정했을 때, 조커와 배트맨을 독립적으로 instance labeling 해야하는 문제가 발생한다. 물론 단순하게 생각해서 각 사람 이미지를 독립적으로 구분하는 작업을 하나씩 라벨링하고 이를 supervision으로 활용해서 학습하게 된다면 얼추 segmentation 성능이 기대하는 것만큼 나오겟지만, 그렇다고 해서 모든 영화의 모든 인물들에 대한 프레임별 labeling을 수작업으로 진행하고 각각을 새로운 label에 매칭하는 것은 <U>학습 과정에서나 데이터셋 구축에서도</U> 시간과 노력이 많이 드는 과정이다.   
기존의 SOTA segmentation 방식은 ImageNet에 대한 pre-trained weight으로 초기화한 representation을 활용하여 segmentation task에 적용하는 한편, 이 논문에서는 CLIP이 가지는 <U>global image representation</U>(Web image scale에 대한 representation)을 활용하고자 한다. CLIP이 ImageNet base로 학습된 representation에 비해 가지는 장점은 다음과 같다.

1. 각 feature에 local image semantic(각각의 feature dimension은 이미지의 일부에 대한 정보를 담는다)을 학습 가능하다. 사실 이 부분은 ImageNet에 pre-trained된 baseline weight의 효과와 거의 유사하다고 볼 수 있다.
2. Open-vocabulary의 concept을 학습할 수 있다. 앞서 설명했던 것처럼 구체적인 labeling 없이도 원하는 object에 대한 segmentation이 가능하다.
3. 물체들 간의 상호작용, 관계 그리고 spatial location에 대한 풍부한 문맥상의 정보를 학습 가능하다.

이러한 CLIP의 장점이 모여 segmentation task에서 CLIP representation의 supervision을 활용할 수 있다면, <U>효율적인 task performance</U>를 기대할 수 있으리라는 것이다.

---

# Failure and success
사실 처음부터 저자들이 실험에 성공한 것은 아니었다. 가장 쉽게 생각할 수 있는 방법은, CLIP 모델 자체도 image encoder로 ResNet 구조를 활용하기 때문에 pre-trained weight을 가져와서 초기화시킨뒤, CLIP image encoder의 weight를 <U>segmentation task에 맞게 fine-tuning</U> 시키면 성능이 좋아질 수 있겠다는 생각을 하게 된다. 예를 들어 segmentation task에는 DeepLab 모델들이 SOTA로 사용되었는데, DeepLab의 weight를 CLIP image encoder의 weight로 초기화시킨 뒤에 backbone을 segmentation에 맞게 학습을 시키는 것이다. 이러한 과정이 CLIP의 text embedding을 굳이 사용하지 않아도 된다는 장점이 있었지만, 이러한 접근법은 CLIP의 장점 중 하나인 <U>unseen class에 대한 segmentation을 진행할 수 있는 능력</U>을 완전히 배제한 것이다.   
따라서 단순히 weight를 초기화하고 fine-tuning을 진행하는 기존 방식에서 벗어나, MaskCLIP이라고 불리는 접근법은 CLIP의 image encoder로부터 추출된 patch-level feature를 활용하여, 기존 CLIP의 마지막 layer였던 attention pooling을 진행하지 않고 CLIP의 text encoder로부터 얻을 수 있는 $1 \times 1$ convolution weight를 기반으로 dense prediction을 진행한다. 사실 여기에서는 convolution 연산이 가지는 의미 자체를 하나의 trick으로 사용한 것인데, 원래라면 기존의 text embedding과 image embedding 사이의 similarity를 통해 classification을 진행하는 task라고 해보면, 사실 이 과정은 특정 prompt에 대한 text embedding과 그에 맞는 image embedding 간의 correlation을 계산하는 것이고, 결국 $1 \times 1$ convolution 연산이 가지는 의미가 각 <U>dimension</U>에 대한 <U>inner production</U>이기 때문이다.   
또한 CLIP baseline 구조를 그대로 사용했기 때문에 MaskCLIP은 좋은 zero-shot 성능을 보였으며, ResNet과 ViT 구조 모두 segmentation에 대해 적용할 수 있었다(이는 <U>attention pooling</U> 연산 구조 자체가 ViT의 <U>image class token</U>이 self-attention하는 과정과 완전히 동일하기 때문). 추가로 segmentation의 성능 향상을 위해 학습할 필요가 없는 **key smoothing**과 **prompt denoising** 과정을 추가하였다. 두 방법에 대해서는 뒤에서 보다 자세히 보고 넘어가도록 하겠다.

---

# MaskCLIP approach
<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214468226-ea549c5c-8af7-4828-8b52-3f01f5fa6f24.png" width="500"/>
</p>

MaskCLIP+로 표시된 부분은 이후에 따로 설명하기로 하고, 우선 MaskCLIP으로 표시된 부분(짙은 회색)을 먼저 확인하면 위와 같다. 참고로 MaskCLIP에서는 학습 과정이 전혀 필요 없고, 단순히 특정 이미지에 대해 얻고자 하는 class label에 대한 text prompt를 만든 뒤, 이렇게 만든 각 class별 text prompt를 encoding한 결과를 $1 \times 1$ convolution의 weight으로 사용하여 <U>image embedding</U>을 <U>dense prediction</U>으로 확장시킨다. 기존 attention pooling이 image feature의 average를 class에 대한 token으로 간주하여 모든 pixel 정보에 대한 attention을 구했던 것과 달리, MaskCLIP에서는 **attention pooling** 이전의 모든 픽셀 정보들을 활용하는 형태가 된다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214469422-31e57d8d-8ba7-4791-93ef-444e269fb30a.png" width="400"/>
</p>

하지만 단순히 이렇게 했을 경우에는 prediction 성능이 크게 좋지는 않았는데, 이유는 아무래도 text prompt와의 similarity를 기준으로 prediction이 진행되는 구조이다 보니, 기존 segmentation task에서와 같이 locality(비슷한 위치의 pixel은 같은 class일 확률이 높다)에 대한 정보가 담기기 힘들고, 무엇보다 feature 단위의 <U>pixel prediction</U>은 noisy한 결과를 보여줄 수 있기 때문이다. 이러한 문제들을 해결하고자 제시한 방법이 바로 앞서 설명을 넘겼던 **key smoothing**과 **prompt denoising** 과정이다.

## Key smoothing
<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214470022-12174ae9-f4d6-4873-a3b0-76114b4e7bf1.png" width="400"/>
</p>

굳이 attention pooling layer가 필요없다면, 결국 key, value값이 더 이상 학습에 관여하지 않게 된다는 의미이다. 그러나 결국 attention layer도 CLIP 학습에 있어 prediction에 영향을 주었을 것이고, segmentation task를 위해 이를 아예 제거하고 본다는 것은 attention layer가 CLIP image encoder의 representation 학습에 끼친 영향력을 아예 무시한다는 것과 같다. 이러면 안되는 것이, attention layer로 하여금 한 픽셀과 다른 픽셀의 연관성을 key value로 학습했을 것이고, 이러한 학습 결과가 각 local semantic에 대해 '<U>어떤 부분들을 참고해야하는지</U>'에 대한 정보를 담았을 것이라고 볼 수 있다. 예컨데 위의 그림과 같이 $K_1,~K_2$ 그리고 $K_3$에 해당되는 feature 영역이 있다고 생각해보자. 서로 다른 물체 영역에 속하는 $K_1$(잔디밭)과 $K_2$(고양이)는 유사도가 낮지만, $K_2$(고양이)와 $K_3$(고양이)는 유사도가 높을 것이다. 

\[
    \text{pred}_i = \sum\_j \cos \left( \frac{k_i}{\parallel k_i \parallel_2},~\frac{k_j}{\parallel k_j \parallel_2} \right) \text{pred}_j   
\]

바로 이러한 key value의 특성을 고려한 prediction이 key smoothing이며, $i$번째 patch에 대한 prediction을 진행할 때, <U>다른 위치의 patch에 대한 key 값과의 유사도</U>를 고려하겠다는 의미가 된다. 참고로 논문에서는 similarity와 곱해지는 $\text{pred}$ 부분의 index가 $i$로 표기되어 있는데, 의미하는 바를 읽어보면 $j$가 맞는 듯하여 위와 같이 식을 수정해보았다.

## Prmopt denoising
<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214471903-e7364181-c82e-4c0d-b6a5-3dc60b7571e1.png" width="400"/>
</p>
또다른 문제는 image에 포함된 class가 너무 많을 경우 발생하는 문제다. 만약 위와 같은 그림에서 'cat', 'grass' 그리고 'tree stump'를 구별하는 segmentation task라고 한다면, 'tree stump'의 경우 다른 이미지의 극히 일부에만 포함되어있어 오히려 key smoothing 과정에서 <U>다른 class에 대한 confidence를 방해</U>하는 역할을 한다. 따라서 본 논문에서는 만약 특정 class에 대한 key similarity가 모든 feature pixel에 대해 $0.5$보다 작을 경우, 해당 class를 prediction 과정에서 아예 배제시키는 방법을 사용하였고, 이를 통해 더 좋은 성능을 얻을 수 있었다고 한다.

---

# Mask CLIP to Mask CLIP+

여기서 멈추지 않고 논문에서는 Mask CLIP 대신 <U>Mask CLIP+</U>라는 방법을 추가로 제시한다. Mask CLIP의 장점은 CLIP base로 zero-shot segmentation이 가능하다는 것이지만, 달리 표현한다면 CLIP architecture 대신 segmentation에 좋은 성능을 보이는 기존 baseline인 <U>DeepLab</U>이나 <U>PSPNet</U> 등등을 사용할 수 없다는 단점이 있다. 특히 [ASPP](https://arxiv.org/abs/1606.00915)와 같은 방식을 사용할 수 없다는 점에서 segmentation task에 특화된 네트워크 설계가 CLIP based method에서는 한정적일 수밖에 없다.   

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214472605-12257674-fbfc-45e4-a386-82f0dbc8444b.png" width="600"/>
</p>

따라서 해당 논문에서는 MaskCLIP+에 대해 위와 같은 framework를 제안한다. 기존 CLIP network는 학습하지 않고, 단순히 MaskCLIP+에 사용될 network가 학습할 수 있는 pseudo-label을 만들어준다. MaskCLIP+에서 사용되는 backbone은 <U>기존 fine-tuning 방식과 동일</U>하게 ImageNet에 대해 pre-trained된 네트워크를 사용한다. 그러나 기존 방식과 다른 점은 fine-tuning 과정에서 사용하는 classifier는 학습하지 않고 CLIP의 text-prompt를 기반으로 생성한 $1 \times 1$ convolution을 사용한다는 것이다. 이를 통해 CLIP이 아닌 다른 network 베이스에도 <U>CLIP space에 대한 knowledge distillation</U>이 가능하다는 접근을 보여주었다.   
다만, MaskCLIP+에서 pseudo label을 계속해서 supervision으로 사용하게 되면 segmentation backbone(dilated backbone)의 성능 upper bound가 CLIP 구조에 한정될 수 밖에 없기 때문에 학습 schedule 상 약 $1/10$ 까지는 CLIP의 pseudo label을 활용하되, 이후에는 성능 수렴이 발생하여 이를 제거하고 <U>self-training</U> 과정을 거쳤다고 한다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214473245-c8549814-b913-4dfd-9b8b-de184aab570f.png" width="600"/>
</p>

그 결과 놀라울 정도로 좋은 segmentation 성능을 보여주었고, 단순히 fine-tuning하는 기존 방식에서 벗어나 CLIP guidance learning 기반의 text prompt의 representation이 <U>supervised based segmentation</U>에 특화된 네트워크 구조에서도 <U>zero-shot segmentation</U>이 가능하게끔 할 수 있는 방법론으로 제시되었다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214473452-a071fb41-b647-4398-867a-68a657778a56.png" width="600"/>
</p>

Fully supervised mIoU가 사실상 <U>zero-shot이 가질 수 있는 최대 한계치</U>라고 볼 수 있는데, MaskCLIP+가 달성한 수치는 기존 SOTA를 뛰어넘어 더 이상 발전이 힘든 수준까지 올라간 것을 볼 수 있다.

---

작성중입니다..^^