---
title: MaskCLIP에 대하여
layout: post
description: Zero-shot, Multimodal learning
use_math: true
post-image: https://user-images.githubusercontent.com/79881119/213900254-5da22304-43ab-457c-9533-0927b0d859df.png
category: paper review
tags:
- Multimodal
- Zero shot
- SSL
- AI
- Deep learning
---

# 들어가며...

사실 리뷰할 MaskCLIP은 ECCV paper와 arxiv paper 두 종류가 있다. 논문 제목은 다르지만 main idea의 제목이 같다보니 조금 혼란스러운 감이 없지 않아 있었다. 처음에 읽고 싶었던 논문은 '[Extract Free Dense Labels from CLIP](https://arxiv.org/abs/2112.01071)'이며, 그 다음에 추가로 arxiv에 올라온 논문 제목은 '[MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining](https://arxiv.org/abs/2208.12262)'으로, 아직 학회에 제출을 하진 않은 것 같지만 느낌상 NeurIPS 2023에 올라올 것 같은 내용이다. ECCV 2022에 올라갔던 논문인 Dense label extraction은 segmentation task에 댇해 CLIP의 zero-shot representation을 어떻게 하면 가장 잘 활용할 수 있을까에 대한 실험을 진행했던 연구였고, 아카이브에 올라온 논문인 self-distillation은 CLIP의 image representation 학습에 self-supervised learning 방식 중 하나인 generative approach(masked autoencoder 방식)을 적용한 연구다. 

---

# CLIP의 장점?
VL(Visual-language) contrastive learning를 통해 vision의 modality인 image와 text modality와의 관계성을 찾은 것이 CLIP 논문이었고, 해당 논문을 기반으로 다양한 dataset의 downstream task(classification, object detection 그리고 segmentation 등등)을 zero-shot 혹은 few-shot 으로 해결할 수 있는 길이 열리게 되었다.
<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214461848-424e4ad3-6f46-4ccf-b7e2-15559865bc6b.png" width="500"/>
</p>
사실상 GPT와 같은 대용량 언어 모델 이외에 computer vision에서 웹 상에서 획득할 수 있는 text prompt 기반 대용량 데이터셋을 학습에 사용한 것은 CLIP이 처음이었으며, 단순히 downstream task를 잘 해결할 뿐만 아니라 language와 vision의 관계를 찾은 것에서 image captioning, view synthesis와 같이 prompt 기반 DL engineering이 가능해졌다는 점에서 <U>멀티모달의 새 시대를 연 장본인</U>이라고 볼 수 있다.

---

# Segmentation에서는 활용할 수 없을까?
ECCV 2022 논문인 'Extract Free Dense Labels from CLIP'는 바로 이러한 CLIP의 장점에 집중하여, 어떻게 하면 CLIP의 zero-shot performance를 segmentation에서도 활용할 수 있을까에 대해 연구를 진행하였다.
<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214462980-32bef7a2-dce7-4c90-bba2-3d18798696f9.png" width="500"/>
</p>
이는 굉장히 큰 의미가 있었던 것이 만약 우리가 위와 같은 그림을 segmentation해야 한다고 가정했을 때, 조커와 배트맨을 독립적으로 instance labeling 해야하는 문제가 발생한다. 물론 단순하게 생각해서 각 사람 이미지를 독립적으로 구분하는 작업을 하나씩 라벨링하고 이를 supervision으로 활용해서 학습하게 된다면 얼추 segmentation 성능이 기대하는 것만큼 나오겟지만, 그렇다고 해서 모든 영화의 모든 인물들에 대한 프레임별 labeling을 수작업으로 진행하고 각각을 새로운 label에 매칭하는 것은 <U>학습 과정에서나 데이터셋 구축에서도</U> 시간과 노력이 많이 드는 과정이다.   
기존의 SOTA segmentation 방식은 ImageNet에 대한 pre-trained weight으로 초기화한 representation을 활용하여 segmentation task에 적용하는 한편, 이 논문에서는 CLIP이 가지는 <U>global image representation</U>(Web image scale에 대한 representation)을 활용하고자 한다. CLIP이 ImageNet base로 학습된 representation에 비해 가지는 장점은 다음과 같다.

1. 각 feature에 local image semantic(각각의 feature dimension은 이미지의 일부에 대한 정보를 담는다)을 학습 가능하다. 사실 이 부분은 ImageNet에 pre-trained된 baseline weight의 효과와 거의 유사하다고 볼 수 있다.
2. Open-vocabulary의 concept을 학습할 수 있다. 앞서 설명했던 것처럼 구체적인 labeling 없이도 원하는 object에 대한 segmentation이 가능하다.
3. 물체들 간의 상호작용, 관계 그리고 spatial location에 대한 풍부한 문맥상의 정보를 학습 가능하다.

이러한 CLIP의 장점이 모여 segmentation task에서 CLIP representation의 supervision을 활용할 수 있다면, <U>효율적인 task performance</U>를 기대할 수 있으리라는 것이다.

---

# Failure and success
사실 처음부터 저자들이 실험에 성공한 것은 아니었다. 가장 쉽게 생각할 수 있는 방법은, CLIP 모델 자체도 image encoder로 ResNet 구조를 활용하기 때문에 pre-trained weight을 가져와서 초기화시킨뒤, CLIP image encoder의 weight를 <U>segmentation task에 맞게 fine-tuning</U> 시키면 성능이 좋아질 수 있겠다는 생각을 하게 된다. 예를 들어 segmentation task에는 DeepLab 모델들이 SOTA로 사용되었는데, DeepLab의 weight를 CLIP image encoder의 weight로 초기화시킨 뒤에 backbone을 segmentation에 맞게 학습을 시키는 것이다. 이러한 과정이 CLIP의 text embedding을 굳이 사용하지 않아도 된다는 장점이 있었지만, 이러한 접근법은 CLIP의 장점 중 하나인 <U>unseen class에 대한 segmentation을 진행할 수 있는 능력</U>을 완전히 배제한 것이다.   
따라서 단순히 weight를 초기화하고 fine-tuning을 진행하는 기존 방식에서 벗어나, MaskCLIP이라고 불리는 접근법은 CLIP의 image encoder로부터 추출된 patch-level feature를 활용하여, 기존 CLIP의 마지막 layer였던 attention pooling을 진행하지 않고 CLIP의 text encoder로부터 얻을 수 있는 $1 \times 1$ convolution weight를 기반으로 dense prediction을 진행한다. 사실 여기에서는 convolution 연산이 가지는 의미 자체를 하나의 trick으로 사용한 것인데, 원래라면 기존의 text embedding과 image embedding 사이의 similarity를 통해 classification을 진행하는 task라고 해보면, 사실 이 과정은 특정 prompt에 대한 text embedding과 그에 맞는 image embedding 간의 correlation을 계산하는 것이고, 결국 $1 \times 1$ convolution 연산이 가지는 의미가 각 <U>dimension</U>에 대한 <U>inner production</U>이기 때문이다.
...작성중