---
title: cs231n 내용 요약 (7) - Regularization, Loss function
layout: post
description: Lecture summary
use_math: true
post-image: https://user-images.githubusercontent.com/79881119/210934049-762c2540-c097-4107-89ba-32bd2b8bc9f3.png
category: deep learning
tags:
- AI
- Deep learning
- cs231n
---

# 들어가며...
바로 이전 게시글에서 batch normalization에 대한 개념과 해당 요소를 실제 딥러닝에서 어떻게 연산하는지 코드를 통해 확인해보았다. 이번 게시글에서는 batch normalization과 같이 정규화 역할을 하지만, layer 사이의 covariance shift를 줄이기 위한 목적보다는 <U>overfitting</U>을 방지하기 위한 목적으로 사용되는 여러 regularization term에 대해 알아보고, 딥러닝에서 다루는 supervised learning task 중 가장 대표적인 **classification**과 **regression**에 대한 loss function에 대해서 알아보도록 하자.   
사실 대부분의 내용은 이전 게시글을 잘 살펴보면 이미 언급한 내용이긴 하지만, 대부분 perceptron이나 linear classifier 등등 설명하면서 보조적으로 곁들인 경우가 많아서 이렇게 따로 다루고자 한다.

---

# Regularization
<U>Regularization</U>이라는 단어가 가지는 딥러닝에서의 의미를 보기 전에 일반적 의미에 집중하면 다음과 같다. **수학**에서나, **통계학, 경제학** 그리고 **컴퓨터 과학**에서의 정규화는 얻고자 하는 정답이 '<U>단순</U>'하고자 할 때 사용한다. 

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/216201990-806c4d39-6157-41e9-abef-9dd42455b559.png" width="400"/>
</p>

Regularization이 사용되는 방법은 크게 두가지로 구분할 수 있는데, 각각을 살펴보면 다음과 같다.

- **Explicit Regularization**이란, optimization 과정에서 explicit term을 더해주는 것이다. Prior, penalty 혹은 constraints가 될 수 있다. 흔히 regularization term이나 penalty term은 optimization function에 cost를 주어 optimal solution을 unique하게 만들어준다.

- **Implicit Regularization**이란 explicit regularization을 제외한 모든 형태의 regularization을 의미한다. Early stopping(학습 도중 적당히 fitting되었다면 멈추는 것)이나 robust loss function을 사용하는 등의 방법이 될 수 있다.

이러한 여러 방법들 중 오늘 살펴볼 $L_1$, $L_2$ regularization은 <U>explicit한 constraints</U>로 적용되는 경우가 많으며(weight decay라는 property로 조절된다), 네트워크 구조상 regularization으로 사용되는 dropout과 같은 방법은 implicit regularization에 해당된다.   
앞서 Linear classifier에 대한 게시글에서도 살펴보았듯, 특정 $W$가 SVM loss를 최적화할 수 있다면 $1$보다 큰 모든 $\alpha$에 대해 $\alpha W$ 또한 같은 조건을 만족하기 때문에 <U>non-unique solution</U> 문제가 발생하고 이로 인해 학습 속도가 저하되거나 수렴하지 못하는 문제가 발생한다고 했었다. SVM loss와는 다르게 softmax와 관련된 task에서 해석했던 내용은 $W$가 커지면 커질수록 각 노드별 output value의 차이가 벌어지게 되고, 이로 인해 복잡한 함수에 수렴하는 neural network가 overfitting될 수 있다고 했다. 그렇기 때문에 결국 weight parameter $W$를 너무 커지지 않도록 조절하는 것이 <U>explicit regularization</U>의 한 방법이 될 수 있고, 이러한 방법들 중 일부를 소개하면 다음과 같다.

## L2 regularization
가장 일반적인 형태의 regularization이다. 모든 parameter value의 squared magnitude를 penalize함으로써, 최소화하는 loss term에 더해주어 weight parameter 또한 줄일 수 있게 해주는 방법이다. 딥러닝 네트워크는 여러 layer와 각 layer를 구성하는 parameter로 연결되어있는데, 이에 대한 regularization term은 loss weight $\lambda$에 대해 다음과 같이 정의할 수 있다.

\[
    \frac{1}{2} \vert \lambda W \vert^2    
\]

Loss term의 앞부분에 $1/2$이 곱해진 이유는 gradient를 구했을 때 weight에 2가 곱해져서 실제로는 $2 \times \lambda$ 만큼의 weight가 최적에 관여하기 때문이다. 해당 term이 포함된 loss function은 다음과 같이 표현할 수 있다.

\[
    \text{Cost} = \frac{1}{n} \sum\_{i=1}^n L(y_i, \hat{y_i}) + \frac{\lambda}{2} \vert W \vert^2  
\]

L2 regularization을 사용하는 regression model을 <U>ridge regression model</U>이라고 부른다. L2 penalize는 직관적으로 보게 되면 peaky한 값들에 더 많은 페널티를 부여한다. 그래서 뒤에 추가로 설명하게 될 L1 regularization보다 이상치에 대한 용인성이 낮다는 특징이 있다. Weight decay, regularization은 training dataset에 지나치게 적응된 weight가 학습되는 것을 방지하는 효과가 생긴다. 예컨데 weight의 <U>특정 node의 값이 지나치게 커져야만</U> training dataset에 대한 성능을 높일 수 있다면, regularization term 없이는 결국 해당 parameter가 지나치게 커지는 양상을 보이게 된다. 이런 문제는 모든 위치의 parameter에 동일한 양상을 보이며, 결국 학습이 완료된 후 parameter는 앞서 본 것과 같이 complexity가 높은 polynomial을 그리게 된다. 일반화의 성능을 높이기 위해서는 <U>training dataset에 대한 overfitting</U>이 아닌, training dataset이 <U>포함된 전체 분포에 대한 정보를 학습</U>해야하기 때문에 정규화를 진행하는 것이 중요하다.

## L1 regularization
위에서는 L2 norm을 사용한 L2 regularization이었고, 이번엔 L1 norm을 사용한 L1 regularization에 대해서 살펴보도록 하자.

\[
    \text{Cost} = \frac{1}{n} \sum\_{i=1}^n L(y_i, \hat{y_i}) + \lambda \vert W \vert
\]

L1 regularization을 사용하는 regression model을 <U>Lasso regression model</U>이라고 부른다. L1 penalize는 앞서 보았던 L2 regularization보다는 이상치에 대해서 더 큰 페널티를 부여하지 않기 때문에, 용인성이 보다 크다고 할 수 있다. 만약 특정 modality를 학습하는 과정에서 이상치에 대한 정보를 유지하면서 학습하고 싶다면 L1 regularization 방법이 좋을 것이고, 그게 아니라 평균적인 weight를 만들고 싶다면 L2 regularization 방법이 더 좋을 것이다.

두 개의 vector가 있다고 생각해보고, 각각의 L1 norm과 L2 norm에 대해 구하면 다음과 같다.

\[
    \begin{aligned}
        v_1 = (0.5,~-0.5,~0) \newline
        v_2 = (0.3,~0.3,~-0.4) \newline
        \vert\vert v_1 \vert\vert_1 = \vert\vert v_2\vert\vert_1 = 1 \newline
        \vert\vert v_1 \vert\vert_2 = \sqrt{0.5} \newline
        \vert\vert v_2 \vert\vert_2 = \sqrt{0.34}
    \end{aligned}    
\]

여기서 알 수 있는 내용은 L1 regularization의 경우 서로 다른 weight parameter에 대해서도 동일한 값을 가질 수 있지만, L2 regularization의 경우 weight parameter가 달라지게 되면 무조건 다른 값을 가지게 되고, L2 norm의 기준은 <U>sparse한 parameter일수록 더 큰 값을 가진다는 것</U>이다. 따라서 **weight parameter**를 기준으로 생각했을때 모델을 sparse하게 구성하고 싶다면 **L1 regularization**을 사용하고, dense하게 구성하고 싶다면 **L2 regularization**을 사용하는 것이 일반적이다.

## Elastic network regularization

물론 L2와 L1 loss를 동시에 사용하는 경우도 있는데, 이를 <U>Elastic network regularization</U>이라고 부르고 다음과 같이 사용한다.

\[
    \text{Cost} = \frac{1}{n} \sum\_{i=1}^n L(y_i, \hat{y_i}) + \lambda_1 \vert W \vert^2 + \lambda_2 \vert W \vert
\]

## Max norm constraints
Infinity norm을 정의하는 방식도 있다. 다만 <U>infinity norm</U>의 경우 norm의 정의에 따라 <U>weight parameter의 최댓값</U>으로 정의되는데, 이를 통해 weight parameter가 가질 수 있는 최댓값을 제한하는 형태의 loss가 된다.

\[
    \text{Cost} = \frac{1}{n} \sum\_{i=1}^n L(y_i, \hat{y_i}) + \lambda \max(W)
\]

---

..작성중
