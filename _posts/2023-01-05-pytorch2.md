---
title: Pytorch 2.0에서 어떤 점이 변했을까??
layout: post
description: New version of pytorch
use_math: true
post-image: https://user-images.githubusercontent.com/79881119/210711363-d6266222-45cb-4fe0-98d3-91e089f5dfc7.png
category: deep learning
tags:
- AI
- Deep learning
- Module
- Pytorch
---

이번 게시글에서 소개할 내용은 pytorch 1.13 버전 이후 새롭게 출시된 [pytorch 2.0](https://pytorch.org/get-started/pytorch-2.0/)을 사용해보는 것이다.

---

# OVERVIEW

개인적으로는 파이토치가 텐서플로우보다 사용하기 편하다고 생각된다. 일단 사용자 친화적(클래스 정의부터 시작해서, 학습 함수를 구현하는 과정 등등이 굉장히 깔끔하다)이기도 하고, 가장 큰 이유는 그냥 내가 많이 써와서인듯하다. 암튼 이렇게 어떤 모듈이 더 낫다는 것은 절대적으로 주관적인 생각이고 파이토치, 텐서플로우와 더불어 다양한 딥러닝 모듈이 존재한다.   
파이토치를 개발한 meta(구 facebook)은 tensorflow와는 다르게 굉장히 꾸준한 문법 및 사용법을 유지해오고 있다. 그런 단계에서 넘어서서, 이제는 next-generation pytorch를 준비하고자 큰 결심을 해서 한 것이 바로 pytorch 1.x에서 pytorch 2.0으로 이름을 달리 지은 것.   
파이토치가 가지는 큰 장점 중 하나는 python API와의 효용성이 크다는 것이다. Pytorch 2.0은 여기에 이어 더 빠른 속도로 학습이 가능하다고 한다. 아마도 Pytorch 2.x 버전에서는 뒤이어 설명할 ```model.compile```이 가장 주요한 변화가 아닐까 싶다.

---

# Pytorch 2.0에서 등장한 model.compile이란?
아마 tensorflow를 사용해본 사람들은 해당 문구에 익숙할 것이다. Tensorflow에서는 ```model.compile```이 의미하는 바가 메소드의 인자로 최적화(fitting)할 optimizer, loss 그리고 metric을 설정해주는 것이다. 여기서 loss는 직접적으로 최적화에 사용되는 objective function으로 작용하고, metric은 classification과 같은 task에서의 accuracy로 생각하면 된다.   
Pytorch 2.x에서의 ```model.compile```은 조금은 다르게, Pytorch가 기존에 작동하던 C++ 베이스에서 넘어와서 python 상에서 구동하도록 한다는 것이다. 그리고 pytorch 2.0에서 좋다고 말할 수 있는 부분은 완전히 추가적인 기능(optional)이므로, ```model.compile```과 같은 기능을 사용하지 않는다고 해서 <U>pytorch 2.x 버전을 사용하지 못하는 것</U>은 아니다. 즉, 이전 버전의 pytorch 모든 코드가 <U>pytorch 2.x에서도 그대로 호환이 가능하다는 것</U>이다. ```torch.compile``` 코드에 적용되는 기반 기술들은 다음과 같다.

- TorchDynamo : Python frame evaluation hooks를 사용하여 pytorch 프로그램 안정성에 기여한다. 정확히는 아직 잘 모르겠지만 백프롭과 같은 graph capture에서 도움이 된다는 듯하다. 
- AOTAutograd : 기존 pytorch의 Autograd 엔진을 오버로딩한다. 미리 연산된 역연산 trace를 생성하기 위해 autodiff를 예측한다. 
- PrimTorch : PyTorch 백엔드를 구축하기 위해 목표로 삼을 수 있는 여러 연산자 집합까지 PyTorch 연산자를 정규화한다. 따라서 PyTorch 기능이나 백엔드의 기입의 장벽이 큰폭으로 낮아지게 되고, 아마 tensorflow와 같이 조만간 경량화 기능도 구현해주지 않을까 기대를 해본다. 사실 이 친구는 TorchInductor에서 딥러닝용 컴파일러를 보조하기 위한 역할로 보인다.
- TorchInductor :  여러 액셀러레이터 및 백엔드용 고속 코드를 생성하는 딥러닝 컴파일러. NVIDIA GPU의 경우 OpenAI Triton을 주요 구성 요소로 사용함.

대충 번역해봤는데, 아무튼 아직 데모 단계에 가까워서 조금 고오급 최신 GPU에 대해서는 성능이 확실하게 보장되는 것 같은데, 옛날 GPU 모델에 대해서는 그렇게까지 유의미한 발전은 아닌 것 같다. 그래도 아마 조금은 빨라질 것 같은 느낌적인 느낌은 있다. 암튼 되게 좋은 점은 <U>기존 파이토치 문법을 그대로 사용해도 된다는 것</U>이다.

---

# Pytorch에서 직접 검증
말만 빠르다 빠르다 하면 검증이 안되니까 실제로 파이토치에서 실험을 해봤다. 내가 한 것은 아니고 파이토치 개발진 분들이 했다고 한다. 앞서 말했던 것처럼 단순히 model을 컴파일만 해주면 되는 부분이라, 163개의 open-source model를 활용(구체적으로 분류해보면 46개의 [HuggingFace Transformer](https://github.com/huggingface/transformers) 모델들, 61개의 [TIMM](https://github.com/rwightman/pytorch-image-models) model들 그리고 56개의 [TorchBench](https://github.com/pytorch/benchmark/) 모델들)하였고, 이는 Image classification부터 시작해서 NLP task나 강화학습과 같이 광범위한 딥러닝 네트워크를 커버한다. 즉, 어떠한 task에 대해서도 속도가 좋아진다는 걸 보여주고 싶었다고 함.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/210737816-cd5c206a-c632-4417-ba8d-f6a3f1a19848.png" width="900"/>
</p>

결론부터 보자면 위와 같다. open-source model의 형태나 코드를 전혀 바꾸지 않고 단순히 ```torch.compile```을 통해 wrapping해주고, 속도 향상과 validate accuracy를 측정하였다. 물론 속도 향상은 data-type에 따라 상이하기 때문에 float32와 Automatic Mixed Precision(AMP)등에 대한 속도 향상을 모두 측정했고, 계산한 두 속도 향상을 $0.75 \times AMP + 0.25 \times float32$로 계산했다고 한다. 여기서 AMP에 weight을 좀 더 넣어준 이유는 실질적으로 더 많이 보여서(활용되어서) 그렇다고 한다.   
163개의 open-source model에 대해 ```torch.compile```은 93%, 모델 학습은 43% 빠른 속도로 동작하였다고 한다. 참고로 이 결과 기준은 **서버용 GPU인 A100**에서 측정된 결과고, 로컬 컴퓨터나 desktop에서 사용하는 GPU인 3090과 같은 시리즈에서는 잘 동작하지 않을 수 있고, 심지어는 더 느릴 수도 있다고 언급했다.

> Caveats: On a desktop-class GPU such as a NVIDIA 3090, we’ve measured that speedups are lower than on server-class GPUs such as A100. As of today, our default backend TorchInductor supports CPUs and NVIDIA Volta and Ampere GPUs. It does not (yet) support other GPUs, xPUs or older NVIDIA GPUs.

실제로 위와 같이 써놓으심.. 잔뜩 기대하고 pytorch 2.0 설치했는데 내 컴퓨터에서는 안된다니 너무 슬펐다. 나중에라도 쓸 수 있겠지 기대하면서 존버해야지

---

# Pytorch 2.0 설치법

우선 자신의 GPU 사양을 확인해준다.

```bash
nvidia-smi
```

나는 CUDA 버전이 11.7이라 해당 스펙에 맞는 latest nightlies를 설치해주었다.

```bash
pip3 install numpy --pre torch torchvision torchaudio --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu117
```

본인 컴퓨터에 CUDA 버전이 11.6이면 다음 install 코드를 쓰면 되고,

```bash
pip3 install numpy --pre torch torchvision torchaudio --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu116
```

그것도 아니고 CPU만 있으면 다음 install 코드를 쓰면 된다.

```bash
pip3 install numpy --pre torch torchvision torchaudio --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cpu
```

---

# Pytorch 2.0 사용법

사실 이건 쓰는게 민망할 정도로 너무 간단하다. 학습 코드가 다음과 같이 구성된다고 가정해보자.

```python
class Model(nn.Module):
		def __init__(self):
				super().__init__()
				self.layer1 = nn.Linear(10, 128)
				self.layer2 = nn.Linear(128, 1)
				
		def forward(self, x):
				x = F.relu(self.layer1(x))
				out = self.layer2(x)		
				return out

model = Model()
model.to('cuda')
for epoch in epochs:
		model.train()
		for data, label in train_dataloader:
				...
				data, label = data.to('cuda'), label.to('cuda')
				output = model(data)
				...
```

여기다가 단순히 ```model.compile```를 추가해주면 된다.

```python
class Model(nn.Module):
		def __init__(self):
				super().__init__()
				self.layer1 = nn.Linear(10, 128)
				self.layer2 = nn.Linear(128, 1)
				
		def forward(self, x):
				x = F.relu(self.layer1(x))
				out = self.layer2(x)		
				return out

model = Model()
model.to('cuda')
model = torch.compile(model) # Only at Pytorch 2.0
torch._dynamo.config.verbose=True 
torch._dynamo.config.suppress_errors = True
for epoch in epochs:
		model.train()
		for data, label in train_dataloader:
				...
				data, label = data.to('cuda'), label.to('cuda')
				output = model(data)
				...
```

사실 슬쩍 코드 몇 줄 추가한게 있는데(dynamo), 이건 지금 지원되는 GPU가 서버용이다 보니 compile만 하면 오류가 나서 error를 막아주기 위한 코드이다. 본인은 서버용 GPU로 돌려보지 않아서 잘 모르겠지만 아마도 지원이 되는 GPU를 기준으로 코드를 돌리면 오류가 없을 것 같다. 사실 실제로 시간 체크해서 더 빠른 결과가 나오는 것까지 올려보고 싶었는데 내 GPU가 1080이어서 오히려 시간이 딜레이되는 현상이 발생했다(pytorch에서 언급했던 것과 같음..).