---
title: cs231n 내용 요약 (1) - Image classification
layout: post
description: Lecture summary
use_math: true
post-image: https://user-images.githubusercontent.com/79881119/210934049-762c2540-c097-4107-89ba-32bd2b8bc9f3.png
category: deep learning
tags:
- AI
- Deep learning
- cs231n
---

# What is image classification?
<U>이미지 분류</U>(Image classification)는 컴퓨터비전(computer vision)으로 해결하고자 하는 여러 가지 task(challenge) 중 가장 기본이라고 볼 수 있다. 기존 머신러닝이 해결하기 힘들었던 데이터 수에 따른 성능 수렴을 해결했던 딥러닝 방식은 이미지 분류 대회에서 AlexNet이라는 네트워크가 우승하면서 시작되었다. 대부분의 사람들은 AI라고 하면 그 시작을 알파고로 기억해주는 사람도 많고, 물론 알파고도 RL 분야에서는 상당히 세상의 이목을 집중시켰던 중요한 이벤트긴 하지만 본인에게는 **AlexNet**이 조금 더 인상깊게 다가온다. 
<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/211229357-29c7e82a-eaa7-4b7e-81ba-81841ed2ec33.png" width="500"/>
</p>
다시 image classification으로 돌아와서, 컴퓨터비전에서 해결하고자 하는 이미지 분류는 간단하게 input으로 특정 이미지를 주면 고정된 카테고리 분류들 중 하나에 매칭하는 것이다. 위의 그림은 CIFAR-10 dataset의 예시로, 총 10가지의 클래스로 구분되며 각 이미지를 잘 나타낼 수 있는 label(dog, cat 등등)로 지표화가 된 상태이다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/211229816-ec1c4c9a-3d7e-4ea7-9f1c-d9af75bf2c66.png" width="500"/>
</p>

이러한 이미지를 인간이 보았을 때는 누가 봐도 <U>고양이</U>지만 컴퓨터의 입장은 좀 다르다. 모든 이미지는 컴퓨팅 환경에서 양자화되고, 3차원 메모리의 형태로 각 픽셀 위치에 따른 RGB value로 매핑된다. 컴퓨터가 처음 고양이 이미지를 받아들였을 때 이 숫자들의 나열을 통해 곧바로 '고양이'라고 추측할 수 있는 가능성은 10% 뿐이다. 따라서 우리는 컴퓨터로 하여금 <U>최대한 다양한 고양이 이미지들의 숫자 배열 사이에서 규칙을 찾아내게 하고 싶은 것</U>이다. Image classification의 개요에 대해 간단하게 짚고 넘어왔다.

---

# What is challenging?
더 깊게 들어가기 전에 우선 텐서(Tensor)에 대한 개념을 언급하는게 좋을 것 같다. Matrix(행렬) 개념을 일반화한 형태인 Tensor(텐서)는 숫자 혹은 데이터의 배열이다. Matrix에서의 rank 개념은 행렬이 가지는 independent vector의 개수이며, 이와 유사하게 Tensor도 rank 개념은 해당 <U>텐서가 가지는 차원 수</U>를 의미한다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/211230352-c1d20e34-770e-4854-9ee2-e2e4aac8ff5c.png" width="500"/>
</p>

만약 어떤 Tensor가 묶음으로 표현될 수 있는 dimension을 $N$개 가지고 있다면, 그 Tensor는 $N$차원의 rank를 가지는 Tensor가 된다. 예를 들어 이미지는 RGB의 채널을 가지는 $H \times W$ 크기의 matrix 모음이기 때문에 다음과 같이 표현할 수 있다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/211230627-b36d86ae-f5ae-471f-82c5-3d409ee87f25.png" width="500"/>
</p>

고양이 이미지의 각 픽셀은 특정 위치에서의 색을 의미하며(보다 구체적으로는 3차원 공간에서 2차원 카메라로 rendering된 색상) 다양한 색이 빛의 삼원색인 RGB의 조합으로 표현이 가능하다. 디지털 카메라 환경에서 RGB 값은 $0 \sim 255$의 값으로 양자화되며(8-bit unsigned), 이는 <U>computing situation</U>에서도 동일하게 적용된다. 따라서 고양이 이미지를 $H \times W$의 spatial dimension을 가지는 matrix를 R, G, B 세 묶음으로 가지고 있는 Tensor로 표현할 수 있다.   
앞으로의 모든 게시글에서 Image의 resolution은 이미지의 <U>spatial dimension</U>을 의미하고, Image의 channel은 이미지의 <U>RGB</U> 축을 의미한다고 생각하면 된다. 결론은 지금 다루고 있는 이미지 분류 task에서 input으로 사용되는 image의 데이터 형태는 3차원의 Tensor로 해석할 수 있다는 것이다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/211231096-d6f43001-9449-4c98-8602-511ac5ec87d2.png" width="800"/>
</p>

하지만 이렇게 데이터로 표현된 이미지에는 <U>큰 문제가</U> 있다. 만약 특정 피사체를 찍는 각도가 바뀐다면(Viewpoint variation), 분명 같은 이미지임에도 불구하고 데이터 상으로는 크게 상이한 결과가 출력될 것이다. 이미지 분류를 위해 사용되는 image의 데이터 형태를 컴퓨터가 받았을 때, 동일한 object임에도 <U>보는 각도에 따른 value 차이가 생겨서</U> 이를 일반화할 수 있는 예측 알고리즘을 찾기 힘들다는 것이다.   
마찬가지로 조도 환경(Illumination conditions)에 따른 데이터 차이도 문제가 된다. 단순히 밤에서 낮으로 바뀐다거나, 아니면 광원의 위치가 바뀌어서 우리가 보는 각도에서의 전경과 후경의 색감에 차이가 생기게 되면 위의 경우와 마찬가지로 같은 장면에 대해 잘못된 예측 알고리즘을 적용할 수 있다.   
이를 제외하고도 scale variation(동일한 사물이지만, 카메라가 가까워지고 멀어짐에 따라 물체의 크기가 다양해질 수 있고 이로 인해 잘못된 예측을 할 수 있음), Deformation(예를 들어 고양이는 <U>액체이므로</U> dynamic한 pose를 보여주는데, 이러한 사물 변형에 대해 robust한 모델링이 힘들다는 것), 물체와 배경 색이 유사해서(보호색) 분류에 차질이 생긴다던지 전경이 후경을 가려서 물체의 구분을 방해한다던지 그리고 같은 class의 물체에 대해서도 다양한 모습이 존재할 수 있다는 문제점 등등 <U>image를 통한 machine learning에 제약이 많다는 것</U>을 알 수 있다. 이런 모든 데이터의 가능성에 대해서 네트워크를 구성한다는 것은 불가능에 가까우며, 여기서 바로 딥러닝의 근간이 될 <U>data-driven algorithm</U>을 해결책으로 사용하게 된다.

---

# Data-driven algorithm
데이터 기반 알고리즘은 <U>귀납적 추리</U>에 가깝다. 최단 경로 찾기 문제나 이진 탐색과 같이 어느 정도 한정된 자원을 가정하고 시작하는 알고리즘과는 다르게 image classification과 같은 task에서 해결하고자 하는 문제는 위에서 설명한 다양한 <U>challenging situation</U>에 무관한 예측이 가능한 알고리즘이다. 단순히 '의자'라는 객체만 하더라도 세상에는 정말 다양한 종류의 의자가 있으며, 심지어 의자의 고정관념을 깨는 예술품들이 등장할 수도 있다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/211232116-3bb0a20c-90ef-4d53-92b8-60d0d0f519aa.png" width="600"/>
</p>

솔직히 말하자면 사람도 오른쪽 그림을 보고 의자라고 말하긴 조금 힘들 것 같은데, 아무튼 여러 조도 환경, 카메라의 회전이나 위치 등등 모든 상황에 대처할 수 있는 알고리즘을 구성하고자 하는 것이다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/211232302-e4a7ba44-0aab-4294-abcb-886ea40e56d6.png" width="600"/>
</p>
적게는 수천장부터, 많게는 수백/억의 데이터를 통해 이를 구현하고자 한다. 최대한의 데이터를 통한 최적화 과정으로, **관측할 수 없는** 세상의 모든 <U>객체 데이터</U>에 적용할 수 있는 알고리즘을 찾는 과정이다. 앞으로 언급할 대부분의 딥러닝 알고리즘은 바로 이러한 방법을 사용하게 된다.

---

# Image classification pipeline

이미지 분류를 앞서 언급한 개념들을 토대로 진행하는데, 총 <U>3개의 구성 요소</U> 혹은 <U>단계</U>로 구분할 수 있다. 그 중 첫번째는 **입력 데이터**이다. $K$개의 라벨(class의 개수)에 각각 매칭되어있는 $N$개의 이미지를 data-driven 최적화 과정에 사용한다. 이를 'Training data'라고 부른다. Supervision(지도) 학습에서는 training data가 이처럼 라벨링이 되어있어야한다.   
두번째로는 학습 단계인 **learning**이다. 이 단계에서는 최적화 과정에서 사용할 목적 함수 혹은 알고리즘을 정의하게 된다. 첫번째 구성 요소였던 training data를 활용하여 최대한 좋은 성능을 내고자 하는 것이 두번째 단계가 된다.   
마지막 세번째는 **evaluation**이다. 최적화 과정에서 실제로 학습 과정 중인 네트워크가 얼마나 좋은 성능을 내는지 확인하는 과정이 필요하고, 이 과정에서 사용될 수 있는 데이터가 필요하다. Training data와 마찬가지로 성능을 확인할 수 있어야하므로 데이터에는 지표화가 되어있어야 하며, 이를 validation/test data라고 부른다. 알고리즘이 예측한 결과에 대응되는 지표는 ground truth라고 부른다.

---

# Nearest Neighbor Classifier
지금 소개하고자 하는 classifier는 딥러닝에서 사용하는 neural network 구조는 아니다. 상당히 비효율적인 알고리즘을 소개할 것인데, 이를 언급하는 이유는 <U>data-driven algorithm</U>이랑 <U>gradient descent algorithm</U>이랑 혼동하지 않는 것이 중요하기 때문이다. 두 알고리즘은 머신러닝과 딥러닝의 관계와 비슷하게 말할 수 있다. data-driven algorithm을 활용한 함수 최적화 방식에 gradient descent algorithm이 있을 수 있지만 두 개념은 <U>서로 다른 개념</U>이기 때문에 꼭 NN형태의 딥러닝이 아니더라도 data-driven approach를 사용할 수 있다. 예를 들어 gradient descent 방식이나 WGAN-GP(Wessertein-GAN with gradient penalty) 방식 등등 현재 딥러닝에서 활용 및 언급되는 모든 최적화 알고리즘은 이전에도 이미 존재했었고, AI 분야가 아닌 통신, 수학 등등에서 주로 사용되던 것이었다.   
Nearest neighbor classifier는 추론을 위해 모든 데이터가 필요하다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/211233597-11a527e0-72e3-49b4-a8cc-1093508f674f.png" width="400"/>
    <img src="https://user-images.githubusercontent.com/79881119/211233625-49c6f4f6-22a9-4059-8028-0a25888c70d8.png" width="400"/>
</p>

왼쪽 그림은 CIFAR-10의 몇몇 샘플을 보여주는 그림이고, 오른쪽 그림은 test image와 비교했을때, training image와 가장 유사하다고 판단한 그림 10장을 보여준 모습이다. 가장 가까운 이웃을 찾는 알고리즘 이름에 맞게 유사함의 정도를 측정하는 메트릭을 정하기만 하면 training image와 test image 사이의 거리를 비교함으로써 nearest neighbor이 매핑된 클래스가 곧 test image에 대해 예측하고자 하는 클래스가 된다는 것. 거리를 계산하는 방법은 다음과 같다.

...작성중