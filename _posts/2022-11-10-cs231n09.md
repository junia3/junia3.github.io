---
title: cs231n 내용 요약 (9) - CNN(Convolutional neural network)
layout: post
description: Lecture summary
use_math: true
post-image: https://user-images.githubusercontent.com/79881119/210934049-762c2540-c097-4107-89ba-32bd2b8bc9f3.png
category: deep learning
tags:
- AI
- Deep learning
- cs231n
---

# 들어가며...
이전까지의 글을 통해 길고 긴 여정을 거쳐 신경망에 대한 내용을 정리할 수 있었다. 딥러닝은 머신러닝의 한 방법론이라고 볼 수 있는 neural network를 확장시켜 보다 깊은 레이어층을 학습하고자 했던 연구였고, 그 연구가 많은 발전을 이루어 현재 AI가 <U>사업의 대부분을 구성하는</U> 세상이 되었다. 사실 딥러닝을 처음 공부했을 때는 공부해도 되는 분야인지 막막하기도 했었고, 공부하다보면 최근 논문이나 연구로 올수록 앞으로 내가 이 분야에서 대학원 생활을 통해 얻을 수 있는게 과연 얼마나 있을까라는 생각을 많이 하게 된다. 그럼에도 불구하고 더 늦지 않게 이 분야를 공부하기 시작했다는 점이 다행스럽다고 생각될 때도 많고 진로를 택한 이후로 '괜히 했다' 같은 후회는 해보지 않았던 것 같다. 아무튼 이번 게시글에서 다룰 내용은 2010년 초중반 이후 활발하게 발전할 수 있었던 딥러닝의 기본인 CNN에 대해서 알아보도록 하고, 도대체 왜 해당 아키텍쳐가 컴퓨터 비전에서 큰 각광을 받을 수 있었는지 정리해보도록 하겠다.

---

# Convolutional neural network
이른바 CNN이라고 불리는 네트워크 구조는 딥러닝에서 자주 활용되는 구조로 알려져있다. 그렇다면 대체 <U>convolutional neural network</U>가 어떻게 딥러닝에서 자주 활용될 수 있었을까? 우선 CNN에 대해서 보기 전에 수학적으로 정의된 convolution 연산에 대해 살펴보면 다음과 같다.

---

# Convolution in $1D$ signal
<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218650535-79cd0b77-cca7-4340-82d8-714a262b76c2.png" width="500"/>
</p>
두 개의 함수 $f$와 $g$가 있을 때, 두 함수의 합성곱을 표현하는 용어가 convolution이며 수학 기호로는 $f \* g$로 표시한다. 합성곱 연산은 두 함수 $f$와 $g$ 가운데 하나의 함수를 $y$축 대칭 및 shift시킨 후, 다른 함수에 곱한 결과를 적분하는 것을 의미한다. 말로는 표현이 어렵기 때문에 식으로 살펴보면,

\[
    f \* g(t) = \int_{-\infty}^\infty f(\tau) g(t - \tau) d \tau    
\]
위와 같이 표현 가능하다. Convolution된 함수를 $t$에 대한 새로운 함수 $h(t)$라고 생각하면, $t$는 $x$축으로 평행이동한 거리를 의미한다. Convolution 연산은 commutable(교환 법칙이 성립)하므로 다음 공식이 성립한다.

\[
    f \* g(t) = \int_{-\infty}^\infty f(t - \tau) g(\tau) d \tau  
\]
그림을 보게 되면 추가적으로 autocorrelation, cross-correlation이라는 연산도 정의되어있는데, 각각은 함수 사이의 연관성을 측정하는 지표로, cross-correlation은 서로 다른 두 함수, autocorrelation은 동일한 함수에 대한 지표로 작용한다. Autocorrelation은 cross-correlation 식과 동일하며 <U>correlation을 구하는 두 함수가 서로 같은</U> 특수한 경우로 생각하면 된다. 연속이며 실수 범위의 신호 $f$와 $g$에 대해서,

\[
    f \star g(t) = f \ast g(-t) = \int^\infty_{-\infty} f(\tau) g(t+\tau) d \tau
\]

위와 같으며,

\[
    g \star f(t) = g \ast f(-t) = \int^\infty_{-\infty} g(\tau) f(t+\tau) d \tau = \int^\infty_{-\infty} g(t - \tau) f(t) d \tau    
\]

convolution과는 다르게 commutable하지 않은 것을 볼 수 있다. 물론 autocorrelation의 경우에는 commutable 특징이 그대로 유지된다. 1차원에서 convolution 연산이 가지는 수학적 의미를 그대로 이해한 채로 CNN을 보게 되면 어떤 점에서 차이가 있고, <U>어떠한 맥락에서</U> convolutional neural network로 정의되었는지 확인할 수 있다.

---

# 다시 돌아와서, CNN
Convolutional neural network는 기존에 다뤘던 모든 neural network 구조와 연산 방법만 다를 뿐 맥락은 서로 유사하다. 이전에 본 perceptron 기반 MLP는 다음과 같은 구조를 가지고 있었다.
<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218655242-ccc0348d-a375-4c3c-a0f4-d371bbb1cee1.png" width="600"/>
</p>
딥러닝 네트워크는 위의 perceptron을 하나의 단위로 여러 node로 구성된 각 layer와 다수의 노드를 포함한 여러 layer로 구성된 구조였다. 즉 학습 가능한 weight와 bias를 가지고 있다고 요약할 수 있다(붉은색으로 표시된 부분). 각 뉴런은 input을 받아들인 뒤 parameter인 weight, bias와 연산을 진행한 후 non-linearity 연산 gate로 표현된 <U>activation function</U>을 거치게 된다. 전체 네트워크는 score라는 output을 내보내는 미분 가능한 함수로서, loss function(SVM/Softmax 등등)을 최소화하는 방향으로 학습된다.
Convolutional neural network도 convolution layer를 일종의 weight, bias가 포함된 neuron으로 생각하면 같은 메커니즘으로 작동한다. 하지만 <U>convolutional neural network(CNN)</U>는 input이 image라는 <U>explicit assumption이 추가되었다는 점</U>이 차이가 되겠다. 이러한 explicit assumption을 통해 architecture가 가질 수 있는 property가 생기게 되었고, 더 적은 parameter 수를 가지고도 높은 성능을 기대할 수 있는 네트워크를 만들게 되었다.

---

# 신경망 아키텍쳐
일반적인 neural network 구조를 보면 앞서 말했던 것과 동일하게 input을 single vector로 받아들인 뒤, 여러 hidden layer를 통해 dimension을 늘이고 줄이는 형태로 변형을 가한다. 각 hidden layer는 neuron 여러 개로 구성되며, 각 뉴런은 이전 layer의 모든 neuron과 fully connected된 구조를 가진다. 그리고 각 layer에서의 node는 서로 연산을 공유하지 않고 독립적인 연산을 진행한다. 마지막 layer는 output layer로써 task에 맞게끔 score를 output으로 도출하는 역할을 한다.   
이러한 MLP(regular neural network) 구조는 차원 수가 급증하게 되면 <U>scalability</U>가 떨어진다. 예를 들어 CIFAR-10과 같은 dataset은 비교적 resolution이 작은 편에 속하는데($32 \times 32 \times 3$), 그마저도 첫번째 hidden layer가 input으로 받아들여야하는 차원 수가 $3072$가 된다. 물론 이 숫자만 보게 되면 그렇게 많지 않은 연산량이라고 생각될 수 있지만 ImageNet 데이터셋과 같이 resolution이 $224 \times 224 \times 3$이 된다면 $150,528$의 weight를 계산해야한다. 그리고 기억해야할 것이, input dimension만 이정도가 되고 output dimension까지 고려하면 weight와 bias에 필요한 parameter의 수가 급증하게 될 것이다. 딥러닝을 구현하기 위해서는 여러 레이어가 필요하고, 충분한 representation을 학습하기 위해서는 각 layer에 충분히 많은 수의 node를 할당해야할 것이다. 따라서 fully-connected layer로 구성한 기존 neural network 구조는 연산 복잡도도 높으며 fully-connected된 특징 때문에 overfitting의 위험성도 높다.
이런 측면에서 3차원의 구조를 가진 neuron, 즉 CNN은 image를 input으로 받아들인다는 explicit한 assumption과 아키텍쳐를 특정 구조로 제한한다는 점에서 얻는 이점이 있다. 기존 네트워크와는 다르게 각 layer는 input에 무관한 3개의 차원 정보인 <U>width, height, depth</U>를 가진다. 예를 들어 input image가 CIFAR-10이라면 input image에 대한 width, height, depth는 각각 $32$, $32$ 그리고 $3$임을 알고있는데, 이렇게 image modality가 가질 수 있는 차원에 대한 정보를 CNN의 각 layer도 가질 수 있다는 컨셉이다.
따라서 연산 구조를 보게 되면 이전의 neural network 연산이 진행되었던 것과 같이 input의 모든 부분을 연산에 포함시키지 않는 것을 확인할 수 있고, fully-connected 구조에서 벗어나면서 image에 대한 연산 overfitting을 방지할 수 있었다. 그리고 연산이 fully-connected manner에서 벗어나 input dimension에 무관하게 진행될 수 있으므로 parameter 수도 감소하게 되었다. 물론 마지막 layer의 <U>output</U>은 task에 따라 기존 <U>neural network의 output size와 동일하게 생성</U>되어야 한다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218660940-dc5beb8a-f507-4480-80bb-5d6d3c49b79c.png" width="400"/>
    <img src="https://user-images.githubusercontent.com/79881119/218660841-e25dd445-4a6e-442c-ad50-41413bb0740c.png" width="550"/>
</p>

---

# How CNN calculate output feature for each hidden layer?

2D convolution 연산 과정을 나타내면 다음과 같다. 예를 들어 고양이 이미지를 분류하는 task에서, $H \times W \times 3$의 고양이 RGB 이미지가 있다고 생각해보자.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218661293-ff18070b-0bff-4728-b450-38b54dac5019.png" width="400"/>
</p>

기존 방식은 위와 같은 RGB image data를 <U>$1$차원 벡터로 확장시킨 후</U> 여러 층의 hidden layer를 통과시키는 과정이었다(좌측 이미지 참고).
<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218661737-1cf74236-6662-4b63-adc6-5e0430005849.png" width="240"/>
    <img src="https://user-images.githubusercontent.com/79881119/218662106-71b39dfc-e798-4532-9142-5bf06e291a10.png" width="640"/>
</p>

그러나 convolution layer에서는 이러한 flatten 과정 없이 원래의 이미지 dimension에 바로 연산이 가능한 convolutional kernel(filter)를 적용하게 된다. RGB 픽셀을 확대해서 나타내보면 우측 그림과 같다. Convolution 연산에서 사용되는 filter의 크기는 input과는 무관하기 때문에, <U>3차원 텐서 형태를 가지는 이미지</U>에 그대로 적용될 수 있다. 각 픽셀 별로 연산에 사용되는 kernel을 표현한 그림이 위와 같다. 각 kernel은 해당 크기에 상응하는 학습 가능한 parameter 수를 가진다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218662688-257853fd-1c41-42e1-9ee3-48d2ea25eadf.png" width="400"/>
</p>

CIFAR-10 dataset에 대한 예시를 입체적으로 표현한 것이 위의 그림과 같다. 옅은 분홍색으로 표현된 $32 \times 32 \times 3$ 크기의 텐서가 input image이고, 그 안에 들어있는 짙은 분홍색으로 표현된 작은 3차원 텐서가 convolution kernel에 해당한다. Convolution kernel은 filter가 커버하는 input 범위에 대한 cross-correlation 연산을 진행하고(자세한 연산은 뒤에서 언급하도록 하겠다) 모든 kernel에 대한 연산 결과를 output dimension에 맞게 추출한 것이 우측에 보이는 파란색 output tensor에 해당한다. Convolution 연산이 진행되는 방식과 convolution 연산 대신 cross-correlation을 언급한 이유에 대해 서술하면 다음과 같다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218663633-9b78b2de-97aa-4421-a45e-e90c562deec1.gif" width="400"/>
</p>

Convolution 연산은 kernel size, stide 그리고 padding이라는 <U>기본 hyperparameter</U>를 가진다. 위의 그림은 padding $= 1$, stride $= 1$ 그리고 kernel size $= 3$인 경우에 해당된다. 청록색으로 표현된 격자가 convolution 연산을 진행한 후 출력된 output을 의미하고, 아래에 있는 파란색의 격자가 input이다. 파란색의 격자 바깥쪽에 점선으로 된 부분이 바로 padding이다.

### Kernel
실질적으로 convolution filter가 <U>input에 적용될 범위</U>를 나타내며, kernel은 필터 자체와 같은 의미를 가진다. 따라서 위의 그림에서는 kernel이란 움직이는 회색 영역에 해당되며, 이때의 kernel size는 $3$임을 알 수 있다. 물론 filter의 kernel size가 무조건 spatial하게 동일해야하는 것은 아니다(가로, 세로가 같은 길이를 가질 필요는 없다).

### Padding
Input에 대해 외곽 부분(점선으로 그려진 부분)이 padding이라고 앞서 이미 설명했었다. Padding은 input 기준으로 constant value를 붙이는 zero-padding과 같은 방법이 있기도 하며, extrapolated padding 등 다양한 방식이 존재한다. Padding $= 1$이라는 의미는 input에 대해 상하좌우 모두 $1$칸씩 spatial dimension을 늘림을 의미하고, 이를 실제로 그림 상에서 확인해볼 수 있다. Padding도 kernel size와 마찬가지로 상하좌우 모두 동일할 필요는 없다.

### Stride
Input에 padding이 추가된 영역을 포함하여 kernel은 stride만큼 움직이며 연산을 진행한다. 위의 예시에서는 $3 \times 3$의 kernel size를 가진 필터가 stride $= 1$씩 옮겨가며 연산을 진행하는 것을 볼 수 있다.

하지만 여기서 짚고 넘어가야할 점은 convolution 연산은 앞서 본 1D convolution 계산과는 다르게 input과 filter가 서로 역방향으로 연산되는 것이 아닌, 같은 방향에서 내적이 이루어지는 것을 확인할 수 있다. 그리고 위의 예시에서는 평면에 대해서 연산이 진행되는 것처럼 보이지만 사실 단일 channel에 대한 연산이 아닌, 모든 channel을 커버하는 하나의 filter가 존재하는 것이다.
<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218683663-ced88b17-4281-49c6-ad45-a6b4fc545117.png" width="1000"/>
</p>

따라서 실제 연산 과정은 위와 같다. 검은색으로 표현한 것이 $3 \times 3$ kernel size를 가지는 filter를 의미하고, stride $= 1$만큼 이동하면서 연산이 진행되는데, 각 채널마다 $3 \times 3$ 크기의 kernel이 적용되기 때문에 filter의 실제 크기는 RGB 채널을 모두 커버하는 $3 \times 3 \times 3$의 3D tensor가 된다. 앞서 convolutional neural network의 각 뉴런은 width, height 그리고 depth를 가진다고 했었는데 지금 보이는 필터의 kernel size가 곧 width와 height를 대표하는 값이며 input image 혹은 feature map의 channel size가 필터의 depth라고 할 수 있다. 연산이 진행되는 과정을 예시로 들면 다음과 같다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218684609-540d8251-15a7-4fbe-a0ee-4c05288e7c3d.png" width="600"/>
</p>

RGB value는 임의로 채운 값이고 필터도 마찬가지로 임의로 채운 값이다. 각 채널별로 필터가 곱해진 뒤 모두 더하는 형태로 inner projection이 진행되며, 이렇게 각 채널별로 더해진 값들이 다시 모두 더해져서 해당 영역에서의 filter가 적용된 output value는 $7 + 15 + 4 = 26$이 된다. 추가로 만약 bias가 존재한다면, output value는 $26 + \text{bias}$ 꼴이 된다. 실제로 필터가 어떠한 역할을 하는지 시각화해보기 위해 toy code를 colab에서 작성해보았다. 우선 본인이 업로드하고 싶은 이미지를 업로드하면 된다. 나는 구글에서 쉽게 얻을 수 있는 아래와 같은 고양이 이미지를 업로드했다. 
<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218698278-4ec7a495-f2c5-4ae4-8eba-5e056e25f357.png" width="400"/>
</p>

이미지를 ```numpy array```로 가져오기 위해 필요한 모듈 그리고 시각화에 필요한 모듈을 정의하였다. 그리고 $0$부터 $255$까지의 값을 가지는 image를 normalize 하였다.

```python
import cv2
from google.colab.patches import cv2_imshow
import numpy as np

image = cv2.imread("cat.jpg")
image = image/255.0 # Normalize image to 0 ~ 1
```

만약 google colab이 아닌 로컬에서 돌리고 싶거나 jupyter notebook을 이용한다면 ```google.colab.patches``` 대신 단순히 ```cv2.imshow``` 메소드를 사용하는 것을 추천한다. 그런 뒤 numpy array에 대한 convolution 함수를 다음과 같이 구성하였다.

```python
def conv2d(image, out_channels, kernel, padding=0, strides=1):
    # Build kernel according to input image size
    image_height, image_width, image_channel = image.shape
    if type(kernel) == int:
        kernel_channel, kernel_height, kernel_width = image_channel, kernel, kernel
    elif type(kernel) == tuple or type(kernel) == list:
        kernel_channel, kernel_height, kernel_width = image_channel, kernel[0], kernel[1]

    kernel = np.random.randn(kernel_channel, kernel_height, kernel_width)

    # Calculate output shape according to input image size and filter size
    output_height = int(((image_height - kernel_height + 2 * padding) / strides) + 1)
    output_width= int(((image_width - kernel_width + 2 * padding) / strides) + 1)
    output_channel = out_channels
    output = np.zeros((output_height, output_width, output_channel))

    # padding(zero-padding) on input
    if padding != 0:
        image = np.pad(image, ((padding, padding), (padding, padding), (0, 0)), 'constant', constant_values=0)
    
    # calculate 2d convolution
    for c in range(output_channel):
        output_per_channel = np.zeros((output_height, output_width))
        for h in range(output_height):
            if (h * strides + kernel_height) <= image.shape[0]:
                for w in range(output_width):
                    if (w * strides + kernel_width) <= image.shape[1]:
                        output_per_channel[h][w] = np.sum(
                            image[h*strides : h*strides + kernel_height, w*strides : w*strides + kernel_height, :] * kernel
                        ).astype(np.float32)

            output[: ,:, c] = output_per_channel

    
    return output
```

예외처리 없이 간단하게 구성하였다. 모든 convolution layer는 정규 분포로 초기화되며, 본인은 총 3개의 convolutional hidden layer가 있는 네트워크를 구상하였다. 각 convolution 사이에는 ```leakyrelu``` 메소드를 다음과 같이 정의하여 activation function으로 사용하였다. LeakyReLU는 $0$보다 작은 value에 $0.1$의 scaling을 주도록 구성하였다.

```python
def np_leakyrelu(image):
    return np.where(image<0, 0.1*image, image)
```

각각의 hidden layer에서의 output을 구하고, 이를 normalize하여 시각화가 가능하게끔 해주었다.

```python
hidden1 = conv2d(image, 3, 3, 1, 1)
hidden2 = conv2d(np_leakyrelu(hidden1), 3, 3, 1, 1)
hidden3 = conv2d(np_leakyrelu(hidden2), 3, 3, 1, 1)

hidden1 = (hidden1 - np.min(hidden1))/(np.max(hidden1) - np.min(hidden1))*255.0
hidden2 = (hidden2 - np.min(hidden2))/(np.max(hidden2) - np.min(hidden2))*255.0
hidden3 = (hidden3 - np.min(hidden3))/(np.max(hidden3) - np.min(hidden3))*255.0
```

```hidden1```, ```hidden2```, ```hidden3```를 모두 시각화하면 다음과 같다.
```python
cv2_imshow(hidden1)
cv2_imshow(hidden2)
cv2_imshow(hidden3)
```

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218700569-99295503-eedf-45e6-8cd4-af6a4cf4a0f0.png" width="600"/>
</p>
필터를 전혀 학습하지 않고 random하게 초기화한 상태로도 어느 정도 물체의 윤곽을 잘 추출해내는 것을 확인할 수 있었다. 만약 task에 따라서 filter가 최적화가 되면, 위에 보이는 이미지보다 image에 대한 feature map을 잘 추출할 수 있을 것이라고 생각되었다.

---

... 작성중