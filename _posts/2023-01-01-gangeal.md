---
title: About GAN supervsed alignment(GAN-gealing)
layout: post
description: GAN gealing
use_math: true
post-image: https://user-images.githubusercontent.com/79881119/210159076-23125484-b1a0-40a4-a676-23518861a952.gif
category: paper review
tags:
- Generative model
- AI
- Deep learning
- GAN
- GAN gealing
---

# 논문 요약 및 소개

이번에 소개할 논문은 이번에 연구실에서 논문을 준비하면서 여러 생성 모델 분야에 대해서 살펴보다가 흥미있게 봤던 <U>GAN-supervised learning</U>이다. 이전 리뷰들은 여러 논문들을 전반적으로 살펴봤었는데, 이번에는 [이 논문](https://arxiv.org/abs/2112.05143) 하나만 집중해서 볼 예정이다. 사실 이름만 봐서는 단순히 generative network를 학습하는 것처럼 느껴지지만, GAN-supervised learning이라는 의미는 'GAN'이 만들어내는 이미지를 supervision으로 사용함과 동시에 최적화하겠다는 의미가 된다.   
[Classical congealing](http://vis-www.cs.umass.edu/papers/PAMI_congeal.pdf) method에 대해서 간단히 말하자면 다음과 같다.

---

# What is congealing?
<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/210159138-2413884d-8cae-4102-9f4d-abf4589c980a.png" width="700" />
</p>
자주 봤던 MNIST(숫자 손글씨) 데이터셋이다. 단순하게 이를 0과 1로 구성된 데이터셋이라고 생각해보자. 같은 사람이 동일한 숫자를 쓰더라도, 해당 숫자는 약간의 픽셀 이동이나 기울기, 숫자의 가로 세로 크기 등등 다양한 형태를 가질 수 밖에 없다. 즉 단일 지표를 나타낼 수 있는 데이터셋의 분포 자체가 퍼져있다는 느낌인 것이다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/210159184-7d57918a-2cd4-41c7-bd61-4bd1ada9111e.png" width="500" />
</p>
위의 그림은 각각 $j^{th}$ image인 $I^j$가 transformation matrix $U^j$에 대해 변형된 이미지 $I^{j\prime}$를 의미하고, 해당 이미지에서의 각 pixel 위치에 대한 index $i$에 대한 지표화를 통해 각 이미지 상에서 동일한 위치 $i$를 $x_i^j$로 표현한 것이다. 모든 transformed digit image를 쌓아두고, 같은 위치의 픽셀들에 대해 empirical entropy(entropy estimation)을 진행한다. 

\[
    \hat{H}(x\_i) = -\left( \frac{N_0}{N} \log \_2 \frac{N\_0}{N} + \frac{N_1}{N} \log \_2 \frac{N_1}{N} \right)
\]

위의 식에서 notation $N_0$과 $N_1$은 각각 이미지 상에서 같은 픽셀 위치에서의 흰색 부분($1$)과 검은색 부분($0$)의 빈도를 나타낸 것이다. 이를 전체 이미지 개수로 나눠주기 때문에 확률값으로 매핑이 되는 것이다. 최적화하고자 하는 방향은 이 엔트로피를 최소화하여, <U>최대한 모든 이미지에 대한 transformation이 같은 형태의 이미지를 만들 수 있게끔</U> 하는 것이다. 

---

# 다시 돌아와서, abstract
이러한 framework 및 ideation 과정을 확장하여 만약 unalign된 데이터셋에 대해 <U>균일한 target mode를 생성할 수 있다면</U>, STN(Spatial Transform Network)를 학습할 수 있는 **supervision**으로 GAN의 mode를 사용할 수 있지 않을까?라는 아이디어가 된다.   
실제로 이 방법을 사용하여 기존 SOTA 기준 precise correspondence(aligned된 부분의 일치하는 정도)가 약 $3 \times$ 증가했으며, 이 방법은 augmented reality나 image editing 혹은 다양한 GAN downstream task에서 학습될 수 있는 이미지 전처리에 활용될 수 있음을 보여주었다.

---

# Why GAN-gealing?
사실 이 부분은 논문의 introduction 부분을 정리하고자 만든 파트인데, 굳이 제목을 '왜 꼭 GAN-gealing이어야만 했는가?'로 지은 이유는, MNIST에서 사용된 기존 congealing 방식을 적용하기엔 복잡한 RGB 이미지에 대해서는 힘들기 때문이다.   
<U>Visual alignment</U>는 워딩에서 볼 수 있듯이 시각적인 정적 영상/동적 영상에 대해 correspondence(서로 다른 이미지에서 동일한 사물 찾기 혹은 위치 적용하기 등)을 찾거나 registration하는 문제로, 주로 computer vision에서 많이 다루는 <U>optical flow</U>, <U>3D matching</U>(depth estimation과 관련됨), <U>medical imaging</U> 혹은 <U>tracking and augmented reality</U>에 적용될 수 있다.   
물론 최근 연구들로 하여금 pairwise alignment(image A로부터 image B)의 alignment는 쉬워졌지만, 여전히 모든 데이터셋에 대한 alignment는 크게 관심받지 못한 분야인 것은 여전하다. 예를 들어, 모든 데이터셋에 대해 자동으로 annotation을 해준다던지, 모든 이미지에 대해서 keypoint를 찾아낸다던지 등등 일반화된 <U>reference frame</U>이 필요한 경우에 대해서는 해결되지 못한 것이다.   
그리고 FFHQ, AFHQ 그리고 CelebA-HQ와 같이 이미 얼굴 인식이나 crop/transform을 통해 <U>align된 이미지에</U> 대해서 generative model이 학습되었을 경우에 <U>더 높은 quality의 샘플을 만들어낼 수 있다</U>는 것이 여러 연구를 통해 입증되었기 때문에, 보다 global keypoint reasoning이나 alignment가 필요하게 되었다.   
이 논문에서는 binary digit과 같은 단순한 modality에서만 적용될 수 있는 기존 congealing 방식의 아이디어에서 보다 넓은 dataset에 적용될 수 있는 GAN-gealing 방식을 제시한다.   
뒤에서 보다 자세히 설명할 것이지만 간단하게 개요만 잡고 가자면 우선 사전 학습된 <U>GAN</U>의 생성 모델을(학습은 unaligned dataset에 대해 진행) 기반으로 random sample과 target sample을 spatial transformer networks(STN)의 supervision으로 활용, <U>STN의 파라미터</U>와 <U>target sample</U>을 동시에 최적화한다. 즉 STN의 학습 과정은 GAN과는 독립적으로(exclusively) 진행된다.

---

# Related works

## GAN 모델의 학습
GAN 종류의 generative model은 classification이나 segmentation, 그리고 representation learning이나 3D graphics에 이르기까지 다양한 computer vision 분야에서 활동되고 있다. 이처럼 해당 논문에서는 deep generative networks의 pre-trained된 joint distribution를 활용하고자 하는 것이다. 그러나 기존 GAN의 학습 과정과 같이 GAN 구조에서 먼저 이미지를 생성한 뒤, 이를 기반으로 discriminative network를 학습하는 것이 아니라, GAN-generated image와 discriminative model을 동시에 학습하는 형태가 바로 GAN-gealing이다. 조금 헷갈릴까봐 말하자면, generative network를 학습하는게 아니라 'GAN generated image' 자체를 최적화하게 된다. 단순히 GAN이 생성한 이미지를 supervision으로 사용한 학습 형태가 되며, 그 어떤 픽셀 단위의 augmentation이나 domain knowledge에 따른 생성 데이터의 후처리를 진행하지 않고 사용한다.

## Joint image set alignment
이미지의 <U>평균</U>이라는 개념은 동일한 semantic content를 가지고 있는 image set에서의 통합적인 alignment를 시각화하기 위해 사용되었다. 대표적으로 지금 리뷰하고 있는 논문의 기초가 되는 congealing에서 unsupervised joint alignment(위에서 언급했듯, 단순히 image 간의 엔트로피를 최소화하는 방향으로 최적화를 진행)하는 방식이 이와 같다. 이러한 방법은 binary digit과 같이 형태가 잘 잡혀있는 데이터 구조에 대해서는 잘 작동하지만, 보다 다양한 형태의 데이터에 대해서는 쉽지 않다. 이런 문제들을 해결하기 위해 이후 연구에서는 고차원의 데이터셋을 low-rank의 subspace로 projection하는 연구나, image를 color, appearance 그리고 shape의 feature로 factorize해서 사용하는 등 데이터 단순화를 통한 연구가 진행되었다.   
모든 연구들은 모든 image를 하나의 mode(데이터셋이 projection되는 <U>또다른 subspace</U>라고 생각하면 됨)로 표현 가능하다고 가정한다. 또다른 연구에서는 joint visual alignment 그리고 clustering 기법을 user-driven data를 통해 해결하였다. Unsupervision으로 alignment를 하는게 아니라 bounding box supervision을 활용하고, object 카테고리에 묶이는 여러 mode에 대해 cluster를 진행한다. 이를 자동화한 방식도 있지만, 특정 domain에만 한정되는 문제가 있다. 이렇게 각 이미지의 optimization을 해결하려한 연구 중에서 warping을 network가 예측할 수 있게 했던 연구가 있고, large-scale collection에 대한 clustering과 alignment를 가능케 했지만, 여전히 simple color transformation에 국한된다는 문제가 있다. 이러한 제약은 complex dataset에 대해서 paper가 가지는 assumption을 무력화한다는 단점으로 작용한다.

## Spatial transformer networks
<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/210162591-a1184fcc-93ff-47f9-bec4-ca6c21ae6cde.png" width="700" />
</p>
다음으로 소개할 내용은 이 논문에서 GAN supervision으로 학습하고자 하는 주요 프레임워크이자, deep learning based data processing 관련 논문으로 유명한 [STN](https://arxiv.org/pdf/1506.02025.pdf)이다. Deep learning framework를 사용하여 학습 가능한 geometric transformation의 파라미터를 학습하는 과정이다. Warpining에 사용되는 몇 개의 parameter를 딥러닝을 통해 학습하고, sampling grid를 input image에 대해 생성하고 warpining하는 과정을 미분 가능한 연산으로 정의하여 학습이 가능하게끔 하였다. STN framework를 모듈로 활용하여 여러 discriminative task에서 좋은 성능을 확인할 수 있었으며, robust filter learning, view-synthesis 그리고 3D representation learning에서 활용되기도 하였다. 이러한 방법들은 공통적으로 STN을 사용하여 또다른 task를 간접적으로 쉽게 만들고자 작용하는 constraint였지만, 이 논문에서는 STN을 직접 학습시키기 위해 GAN network를 사용한다는 점이 다르다. 

---

# GAN 기반 supervised learning
<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/210162569-06a79464-3c31-48f9-9831-ce2ddb04e6a2.png" width="800" />
</p>
이제 본격적으로 해당 논문의 학습 방법에 대해 자세히 살펴보도록 하자. 설명에 사용할 supervision pair $(x,~y)$는 기존 supervised learning에서 사용했던 바와 같이 source-target 관계에 놓여있다고 생각하면 된다. 우리는 <U>'STN'</U>을 학습시킬거니까, **source image**는 정렬이 안 된 이미지(위의 그림에서 아무렇게나 위치한 고양이)라고 생각하면 되고, **target image**는 정렬이 된 이미지(위의 그림에서 우측과 같이 잘 정렬된 고양이)라고 보면 된다. 이렇게 정렬된 데이터셋을 구성하는 과정에서 GAN generator가 생성한 이미지를 사용하게 되는 것이다.   
먼저, $x$는 미리 학습된 GAN generator로부터 임의로 생성하는 sample이다. 그리고 $y$는 $x$의 latent code에 추가적으로 가해진 latent manipulation를 통해 생성된다.이렇게 pair는 STN network $f_\theta : x \rightarrow y$를 학습하는 supervision으로 작용한다.

\[
    \mathcal{L}(f\_\theta,~y) = l(f_\theta(x),~y)    
\]

여기서의 $l$은 reconstruction loss이다. 말 그대로 정렬되지 않은 데이터 $x$를 정렬된 데이터 $y$로 만들고자 하는 것. 일반적인 supervised learning에서는 여기서 데이터 $x$와 $y$가 고정이지만, 신기하게도 GAN supervised learning에서는 고정된 데이터를 STN($f_\theta$) 최적화에 사용하는 것이 아니라 <U>target이 되는 $y$ 또한 end-to-end로 최적화된다</U>. 학습이 끝나고 나면 STN network $f_\theta$를 사용하여 GAN generated image가 아닌 real input에 대해 테스트하게 된다.

## Dense visual alignment
그렇다면 도대체 어떤 방식으로 congealing을 진행해야 GAN-supervision을 사용할 수 있을지 알아보도록 하자. 이제부터 GANgealing이라고 불리는 알고리즘에 대한 엄밀한 분석이 들어간다.   
GANgealing은 가장 먼저 latent variable generative model $G$를 unaligned input dataset에 대해 학습하는 것으로 시작된다. 이때, input이 되는 latent vector는 $w \in \mathbb{R}^{512}$로 표현한다. 왜 $z$가 아니냐고 물어본다면, 나중에 말하겠지만 여기서 사용되는 generator는 styleGAN 기반이기 때문이다. 암튼 이렇게 학습된 $G$가 있다면, 여기서 source가 되는 input sample은 그냥 알고 있는 latent space $w \sim \mathcal{W}$에서 추출하고, generator의 생성된 샘플 $x = G(w)$로 정의하면 된다. 이제, 반대로 target image가 되는 $G(c)$를 정의한다. 여기서 $c$는 latend vector로 input에 사용된 $w$와 같은 dimension이 되며, 이를 STN의 target으로 사용하게 된다. 여기서 드는 의문점은 '$w$는 랜덤으로 뽑으면 되는데 $c$는 source가 되는 G(w)에 맞는 latent를 골라야하는데 어떻게 해요?'인데, 결국 앞서 말했던 것과 같이 $c$를 STN network 학습과 더불어 최적화하는 것이 목적이고, generator $G$는 input $c$에 대해 미분 가능한 함수이므로 gradient descent 방식을 사용해 학습 가능하다.

\[
    \mathcal{L}\_\text{align}(T,~c) = l(T(G(w)), G(c))    
\]

여기서 $T(\cdot) = f_\theta(\cdot)$으로 생각하면 된다. $l$은 두 이미지 사이에 작용되는 distance metric이 된다. 그림 상으로는 <U>perceptual loss</U>를 사용한 듯하다. 아무튼 이걸 latent vector $c$에 대해서 최소하하는 과정은 결국 <U>latent $c$로 하여금</U> 모든 생성된 이미지 데이터 $x$가 <U>spatial transformer($T$)를 통해 가까워질 수 있는 샘플</U>을 찾고자 하는 과정과 같다. 즉 $G(c)$는 모든 이미지 데이터 $x$의 alignment를 생성하고자 하는 목적으로 작용한다는 것. 만약 처음 $c$가 이런 조건을 만족하지 못하고 $T$를 통해 도저히 도달할 수 없는 이미지를 만들어낸다면, 이는 loss function의 작용에 의해 자동으로 최적화될 것이고, 같은 iteration을 계속 반복하다보면 어느샌가 $c$는 평균 이미지를 잘 생성할 수 있게 된다는 것.   
이러한 단순한 접근은 상당히 그럴듯해 보이지만 dataset의 diversity가 적은 경우에만 가능하다는 단점이 있다. 왜냐하면 아무리 학습되더라도 모든 input image $x$는 같은 constant latent $c$에 의해 생성된 이미지 $G(c)$를 타겟으로 학습되기 때문에 최적화에 한계가 있다는 것이다.   
아마 GAN inversion에 대해 실험해본 사람이면 알겠지만, 무작위로 sampling한 latent를 최적화하는 것보다는 <U>input image에 대해 가까운 sample를 기반으로 최적화하는 것</U>이 생성 이미지의 퀄리티를 높이기 좋은 걸 알 수 있을 것이다. 바로 요 아이디어를 적용하였다. 단순히 same target $G(c)$를 모든 randomly sampled image $G(w)$에 사용하는 것이 아니라, pose 및 orientation은 샘플 간에 유지하면서 디테일한 appearance는 G(w)를 가져가고 싶었기에, 이를 적용한 latent를 최적화한다. 사실 이 부분은 내가 작성했던 image manipulation글과 StyleGAN based GAN inversion 글을 보면 이해가 빠를텐데, StyleGAN에서 이미지를 만들어내는 방식이 <U>low resolution부터 latent를 constant에 입혀가면서 coarse style부터 점차 fine detail까지 생성하는 과정</U>이다. 따라서 적절한 부분에서 target vector $c$와 input sample을 생성할 때 사용한 random latent $w$를 조합하면, $\text{mix}(c, w) \in \mathbb{R}^{512}$가 곧 $c$의 전반적인 형태를 유지한 채로 $w$가 만드는 이미지의 디테일을 살리고자 하는 것이다.

\[
    \mathcal{L}\_\text{align}(T,~c) = l(T(G(w)), G(\text{mix}(c,~w)))
\]

실제 실험에서는 StyleGAN2를 generator로 사용했으며, 이를 사용함에 따라 StyleGAN이 가지고 있는 style-pose disentanglement를 활용할 수 있었다고 한다.

..작성중