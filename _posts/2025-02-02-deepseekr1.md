---
title: 딥시크(Deepseek)-R1에 대한 고찰. Thinking about Deepseek-R1 with Reinforcement Learning(RL).
layout: post
description: Deepseek LLM
use_math: true
post-image: https://github.com/user-attachments/assets/0a323aae-04db-4373-97d9-ee176a05358e
category: paper review
tags:
- Deepseek
- Reinforcement Learning
- LLM
---
# A Little Bit Personal

### 들어가기전 극히 개인적인 주저리

나는 아직도 공부를 한다. 대부분의 개발자는 커리어를 쌓으며 끊임없는 공부가 필요한데, 왜냐하면 지금까지 익혀온 기술 스택이 더이상 트렌디하지 않아지는 경우가 많기 때문이다. 특히나 AI의 경우에는 빠른 변화를 보이는데, 그래서 그런지 실제로 논문을 쓰다가 미친 일반화 성능을 보이는 파운데이션 모델이 나와버리면 해당 task가 아예 날아가버리는 경우도 발생하기 시작했다. 이렇듯 AI 연구자의 숙명은 어쩔 수 없이 기술의 최전선에서 그 누구보다 빠르게 현황을 파악해야만 살아남는다는 사실이다.

<p align="center">
    <img width="372" alt="Image" src="https://github.com/user-attachments/assets/b369e902-73c8-4dee-bd34-fc8cc57b2c8e" />
    <img width="278" alt="Image" src="https://github.com/user-attachments/assets/6c89dc94-ce72-407e-a6d0-90ea34ac33bc" />
</p>

본인도 학부 그리고 대학원 시절을 거치며 개인적으로 새로운 인공지능 논문들이 나올 때마다 거의 즉각 읽는 습관을 들였다. 처음에는 일주일에 한 편 읽는 것도 어려웠는데 이제는 완전 디테일하게 읽지는 못해도 하루에 논문 두세편 정도는 읽어낼 수 있게 되었다. 인공지능을 연구하는 사람이기도 하고 나에게 있어 논문은 일종의 뉴스, 신문같은 존재이며, 급변하는 기술 환경에 보다 빠르게 적응할 수 있도록 도움을 주는 방법이다. 딥시크라는 모델도 테크니컬 레포트로 처음 접하여 알고는 있었지만 R1 모델의 등장이 이렇게나 파격적인 행보를 보일 줄은 몰랐다.

나름 챗지피티 이후로도 인공지능 모델은 끊임없이 학계나 산업계에서 연구되었으나, 그 모든 연구들이 딱히 크게 이슈화되는 경우는 거의 없었다. 여기서의 이슈화는 사람마다 기준이 다르겠지만 적어도 본인은 지금의 scaling law를 깨부술 무언가를 원했던 것일수도 있다. 물론 다양한 연구들이 진행되었기 때문에 현존하는 수많은 closed/opened source AI의 성능 향상에 큰 영향을 끼친 것은 사실이다.  그러나 실제 서비스에 활용되는 모델을 기준으로 대부분의 연구는 덩치 키우기에 집중했다. 거대한 자본이 수많은 리소스와 컴퓨팅 파워, 리소스 그리고 Human Resource(AI Engineer)을 끌어모았고,  매우 빠른 연구 및 개발이 시작되었다. Explainability와 scalability의  끊임없는 줄다리기가 어느새 scalability의 승리굳히기처럼 됐다. 그로 인해 인공지능 연구 방향이 바뀌기도 했으며, 기존에는 생각하지도 못했던 task가 새롭게 제안되는 일도 생겼다.

사실 테크니컬 레포트를 읽어본 후기로는 딥시크는 <u>기술적으로 그렇게까지 독보적이거나 유니크한 모델이라고 볼 수 있을까?</u>였다. 그럼에도 대단하다고 여긴 것은 현재의 기술 시장을 흔들 정도의 파급력을 가져왔다는 사실이고, 기존에 소스코드를 공개하지 않았던 OpenAI의 기술력을 따라잡기 위해 정말 많은 노력을 했다는 사실이다. 적어도 적당히 타협해서 오픈 소스 튜닝하던 대부분의 방법들보단 훨씬 유의미한 결과를 낸 것은 사실이다.

아무튼 이제 글을 시작해보고자 한다. 이번 글은 테크니컬 라이팅이라기 보다는 개인적인 고찰, 혹은 일기장 정도 될 것 같다. 

# Preliminary

### 강화학습에 대한 고찰

우리는 로봇에게 어떤 일을 수행하게 시키고 싶다.

<p align="center">
    <img src="https://github.com/user-attachments/assets/2c50c2a0-e5f6-4edc-a64e-28bc8d274fc8" width="500" />
</p>

지금은 딥시크, 즉 LLM 모델에 대해 얘기하고 있으니 LLM 모델을 기준으로 말하면 <u>‘대답 잘 하게 만들기’</u> 쯤 되겠다. 옛말에 미운놈 떡 하나 준다는 말이 있는데 인공지능의 세상은 그리 호락호락하지 않다.

앞으로 우리는 <u>예쁜놈한테 떡 하나를 더 줄거다</u>. 이러한 개념을 “Reward(보상)”이라고 한다. 적절히 잘 학습된 모델을 가지고 이런저런 질의응답 Environment(환경)에서 대답을 잘하면 칭찬해주는 프로세스를 반복해서 <u>더욱 대답을 잘하는 모델을 만들고자 하는 것</u>이다.

### Policy based method의 발전

기존의 강화학습 방식이었던 Value based method는 인공지능 모델이 취할 모든 행동에 대해 가치평가하는 모델을 학습시킨다. 그래서 인공지능 모델이 현재 상태에서 어떤 Action(동작)을 취할지에 따른 Value(가치)를 판단하고, 가치에 따라 greedy 알고리즘으로 다음 동작을 결정하는 방식을 학습하게 된다. 이때 각 상황에서 어떤 동작을 취할지에 대한 기준이 바로 Policy(정책)이다. 이때 <u>가치를 판단해주는 모델을 딥러닝으로 학습</u>시키자는 관점이 바로 <u>DQN(Deep Q-Network)</u>이다.

이때 Value function에 해당되는 Q-Network의 개념이 나온 이유는 여러 동작에 대한 Reward가 가지는 불안정성을 어느 정도 <u>action-value function Q</u>가 해소해줄 수 있다는 관점이었다 (아래는 알고리즘 코드).

<p align="center">
    <img src="https://github.com/user-attachments/assets/6af4d508-2276-4820-ae4e-150e8a994b9c" width="700" />
</p>

그러나 사실 생각해보면 결국 가치 판단을 위한 모델을 학습하는 것의 최종 목적은 <u>‘각 동작의 가치를 잘 판단하자’</u>가 아니라 <u>‘가장 높은 가치를 지니는 동작을 취하게 하자’</u> 라는 것이다. 그렇다면, 그냥 모델이 취할 모든 행동에 대한 가치 판단 없이 바로 모델의 현재 상태에서 바람직한 다음 동작을 예측하는 Policy, 그 자체를 학습하는 것이 낫지 않겠는가에 대한 근본적 의문이 생긴다. 심지어 동작이 속한 공간을 Discrete하게 만들 필요도 없다. 정책 모델이 내뱉은 확률 분포에 근거해서 다음 동작을 예측하고, 만약 이러한 흐름이 좋은 결과를 가져왔을때 보상을 주어 Policy 모델을 업데이트한다. 이러한 학습법에 대한 이론이 바로 <u>‘Policy Gradient’</u>이다.

강화학습에서는 다양한 시뮬레이션  환경을 가정한다. 데이터셋 전체를 objective function에 empirical하게 근사시키는 deep learning의 개념과 유사하게, 강화학습에서는 각 시뮬레이션 단계를 ‘Episode’라고 부르고 하나의 에피소드 내에서 시간의 흐름에 따라 강화학습을 구성하는 각 모델의 입출력이 나오게 된다. 결국 강화학습의 가장 큰 목적은 다양한 에피소드로부터 모델의 다양한 의사결정방식을 받고, 해당 모델이 <u>가장 이상적인 의사결정방식을 취했을 때</u> 모든 에피소드로부터의 리워드는 가장 큰 기댓값을 가지게 된다. 흔히 정책은 $\theta$라는 매개변수를 가지는 함수 $\pi_\theta(\cdot)$로 주로 표현하게 된다. 이 정책 함수는 각 state $s$에서 수행할 수 있는 모든 액션 $a$에 대한 확률 분포를 추출할 수 있다.

따라서 우리는 특정 매개변수를 가지는 정책(Policy)모델이 특정 액션을 수행했을때 관측 가능한 입/출력 결과에 대해 최대의 reward를 얻을 수 있는 방향으로 모델을 학습하고자 하며, 이 방향이 곧 <u>policy gradient</u>에 해당된다.

사실 policy의 gradient를 계산하는 과정은 순탄치 않다. 그 이유는 reward function에 대한 gradient는 정책 함수($\pi_\theta$)가 어떤 액션을 수행하였는가 뿐만 아니라, 실제로 마르코프 프로세스에서 정책 함수에 의하여 불가피하게 조정된 현재 state $s$에 대한 확률 분포 또한 고려되어야하기 때문이다. 즉, 정책 함수에 추가로, 정책 함수로 하여금 지속적으로 변해온 state $s$의 확률 분포에도 $\theta$가 기여한 바가 있기 때문에 gradient를 직접 구하기 어렵다는 문제가 발생한다. 그러나 이를 단순히 무시할 수 있다는 이론이 바로 <u>policy gradient theorem</u>이고 한 문장으로 다음과 같이 정리할 수 있다.

\[
\text{보상 함수의 gradient는 정책 함수의 gradient에 비례한다.}
\]
엄밀한 증명은 아래에 간단하게 요약해보겠다.

\[
\nabla_{\theta} J(\theta) = \nabla_{\theta} \sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}} Q^{\pi}(s,a) \pi_{\theta}(a|s) \\
\propto \sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}} Q^{\pi}(s,a) \nabla_{\theta} \pi_{\theta}(a|s)
\]

보상함수는 각 state에 머물 확률과, 그 확률에서의 모든 액션 수행에 대한 Value의 총합(혹은 기댓값)을 의미한다. 위의 식에서 <u>뒷쪽에 있는 state value function</u>에 대한 gradient를 구하면 다음과 같다.

\[
\begin{aligned}    \nabla_{\theta} V^{\pi}(s) &= \nabla_{\theta} \left( \sum_{a \in \mathcal{A}} \pi_{\theta}(a|s) Q^{\pi}(s, a) \right) \newline    &= \sum_{a \in \mathcal{A}} \left( \nabla_{\theta} \pi_{\theta}(a|s) Q^{\pi}(s, a) + \pi_{\theta}(a|s) \nabla_{\theta} Q^{\pi}(s, a) \right) \newline    &= \sum_{a \in \mathcal{A}} \left( \nabla_{\theta} \pi_{\theta}(a|s) Q^{\pi}(s, a) + \pi_{\theta}(a|s) \nabla_{\theta} \sum_{s', r} P(s', r | s, a) (r + V^{\pi}(s')) \right) \newline    &= \sum_{a \in \mathcal{A}} \left( \nabla_{\theta} \pi_{\theta}(a|s) Q^{\pi}(s, a) + \pi_{\theta}(a|s) \sum_{s', r} P(s', r | s, a) \nabla_{\theta} V^{\pi}(s') \right) \newline    &= \sum_{a \in \mathcal{A}} \left( \nabla_{\theta} \pi_{\theta}(a|s) Q^{\pi}(s, a) + \pi_{\theta}(a|s) \sum_{s'} P(s' | s, a) \nabla_{\theta} V^{\pi}(s') \right)\end{aligned}
\]

우선, 전체 식에 대한 gradient는 product rule에 따라 분배된다. 이때 뒤쪽에 있는 $Q^\pi$는 <u>현재 가치와 미래 가치의 합</u>으로 표현 가능하다.  이때 뒤에 발생하는 $P$는 Markov decision process(MDP)에 따른 확률 분포로 생각하면 되고, 이전 state가 $s$일때 모든 다음 state에 대한 확률을 정의할 수 있다. 이는 어떠한 정책 함수에 따른 결과가 아니기 때문에 $\theta$라는 변수와 독립이다. 따라서 뒤쪽 term에 있는 gradient는 시그마의 안쪽으로 들어갈 수 있게 되며, 최종적으로는 $P(s^\prime, r \vert s, a)$를 $r$에 대해 marginalize하면서 수식이 완성되는 구조다.이 수식에서 주목할 점은, <u>다음 state value function의 gradient가 현재 state value function에 recursive하게 들어간다는 사실</u>이다. 그렇다면 뒤쪽에 들어있는 $\nabla_\theta V^\pi (s^\prime)$ 또한 다음 state인 $\nabla_\theta V^\pi (s^{\prime\prime})$의 recursive한 수식으로 표현된다.  이렇게 계속 unrolling(recursive하게 $s^\infty$까지 전개)한다고 생각해보자. 

아래의 식에서 $\rho$는 policy function $\pi$에 의해 $k$번의 step 이후 특정 state로 바뀔 확률을 간소화하여 표현한 식이다.

\[
\begin{aligned}
    \nabla_{\theta} V^{\pi}(s) &= \phi(s) + \sum_{a} \pi_{\theta}(a|s) \sum_{s'} P(s'|s,a) \nabla_{\theta} V^{\pi}(s') \newline
    &= \phi(s) + \sum_{s'} \sum_{a} \pi_{\theta}(a|s) P(s'|s,a) \nabla_{\theta} V^{\pi}(s') \newline
    &= \phi(s) + \sum_{s'} \rho^{\pi}(s \to s', 1) \nabla_{\theta} V^{\pi}(s') \newline
    &= \phi(s) + \sum_{s'} \rho^{\pi}(s \to s', 1) \sum_{a \in \mathcal{A}} \left( \nabla_{\theta} \pi_{\theta}(a|s') Q^{\pi}(s', a) + \pi_{\theta}(a|s') \sum_{s''} P(s''|s', a) \nabla_{\theta} V^{\pi}(s'') \right) \newline
    &= \phi(s) + \sum_{s'} \rho^{\pi}(s \to s', 1) \left( \phi(s') + \sum_{s''} \rho^{\pi}(s' \to s'', 1) \nabla_{\theta} V^{\pi}(s'') \right) \newline
    &= \phi(s) + \sum_{s'} \rho^{\pi}(s \to s', 1) \phi(s') + \sum_{s''} \rho^{\pi}(s \to s'', 2) \nabla_{\theta} V^{\pi}(s'') \quad \newline
    &= \phi(s) + \sum_{s'} \rho^{\pi}(s \to s', 1) \phi(s') + \sum_{s''} \rho^{\pi}(s \to s'', 2) \phi(s'') + \sum_{s'''} \rho^{\pi}(s \to s''', 3) \nabla_{\theta} V^{\pi}(s''') \newline
    &= \dots \newline
    &= \sum_{x \in \mathcal{S}} \sum_{k=0}^{\infty} \rho^{\pi}(s \to x, k) \phi(x)
\end{aligned}
\]

우리는 현재 state에서 미래 state $x \in \mathcal{S}$로 가는 모든 MDP를 전개했다. 이제 전개된 식에 state의 초기 상태를 대입하고 여러 전개 과정을 거치면 다음과 같다.

\[
\begin{aligned}    \nabla_{\theta} J(\theta) &= \nabla_{\theta} V^{\pi}(s_0) \newline    &= \sum_s \sum_{k=0}^{\infty} \rho^{\pi}(s_0 \to s, k) \phi(s) \newline    &= \sum_s \eta(s) \phi(s) \newline    &= \left( \sum_s \eta(s) \right) \sum_s \frac{\eta(s)}{\sum_s \eta(s)} \phi(s) \newline    &\propto \sum_s \frac{\eta(s)}{\sum_s \eta(s)} \phi(s) \newline    &= \sum_s d^{\pi}(s) \sum_a \nabla_{\theta} \pi_{\theta} (a | s) Q^{\pi}(s, a)\end{aligned}
\]

이를 통해 <u>정책 함수의 gradient의 방향</u>이 곧 <u>보상 함수의 gradient의 방향</u>과 일치한다는 점을 알아낸 것이다.

### TRPO to PPO

앞서 본 내용은 policy gradient에 대한 기본적인 내용이었다. 그러나 이 이후 사실 LLM에 적용되기까지 강화학습 분야에서 policy gradient에 대한 다양한 접근법이 있었으며, 지금부터 살펴볼 내용이 현존하는 딥시크-R1의 아이디어 근간이라고 할 수 있다. TRPO와 PPO 중 [PPO](https://arxiv.org/pdf/1707.06347)가 조금 더 중요한데(**OpenAI에서 쓴 논문**), TRPO를 꼭 짚고 넘어가야하는 이유는 <u>TRPO</u>(**피터 아벨 연구소 논문**)가 가장 기본 틀이 되기 때문이라고 생각했기 때문이다. TRPO에는 수학적으로 어려운 내용이 포함된다. 가장 간단하게 설명하자면 TRPO는 정책 모델을 학습할 때 “신뢰 가능한 구간 내에서 업데이트한다”라는 개념이다. 그렇다면 대체 왜 신뢰 가능한 구간이 중요한 것일까?

Policy gradient은 다음과 같이 진행된다. 초기 상태는 확률 분포 속에서 샘플링되었다고 가정하자. 예를 들어 우리가 걷기 시작하거나 뛰기 시작할때 항상 같은 자세에서 시작하지는 않는 것과 같다.  우리는 시시각각 지금 행동에 기반하여 다음 행동을 결정하고 이를 수행한다. 이러한 과정을 우리는 자연스럽게 처리하지만 로봇에서 시키는 경우를 생각해봐야 한다.

로봇이 지금 현재 어떤 행동을 수행했을때, 그 행동이 가져오는 가치 그리고 리워드를 수치화하고 이를 누적해서 더해간다. 그리고 그 행동에 기인하는 Policy가 지속적으로 좋은 방향으로 학습된다면, 이론상 업데이트되는 정책에 따른 누적된 리워드는 계속 우상향할 것이다.

Optimal solution에 수렴할 때까지 지속 학습을 진행한다고 가정해보자. 모든 state에서 평가된 가치가 0보다 크거나 같다면 결국 모든 미래가치가 0이 되어 더이상 발전할 수 없는 상황이 올 때의 Policy가  최적의 해가 될 것이다. 하지만 업데이트된 policy($\tilde{\pi}$)는 모든 다음 state, action($s, a$)에 따른 advantage($Q^\pi$)를 0보다 같거나 크게 만든다는 보장이 없다. 이를 다소 단순화하기 위한 방법으로 학습하고자 하는 Objective는 다음과 같이 표현된다.

\[
L(\tilde{\pi}) = \eta (\pi) + \sum_s d^\pi (s) \sum_a \tilde{\pi}(a \vert s) Q^\pi(s, a).
\]

이제는 state에 대한 density 변화($d^{\tilde{\pi}}$)는 무시할 수 있다. 좌측 함수와 우측 함수의 초기값(파라미터 : $\theta_0$)이 동일하기 때문에 Locally 아주 작은 변화량 $\Delta \pi$에 대해서 gradient를 같은 방향으로 유지할 수 있지만, step size를 키울 수 없다는 문제가 생긴다. 만약 gradient가 이상적으로 생기지 않았다면, step size를 잘못 지정해주면 학습이 불안정해질 수 있다. 실제로 많은 강화학습에서 가장 큰 문제가 이러한 학습 불안정성에서 기인하며, 이를 해결하기 위해 수많은 방법론이 제안되었다.

이를 해결하는 방법은 새로운 policy를 기존의 policy와 $\alpha$ mixing하게 되면 lower bound를 가진다. 근데 이 $\alpha$값도 매 task마다 새롭게 정의하기 힘드니, $\pi$와 $\tilde{\pi}$ 간의 거리 메트릭으로 정하자는 것 ($\alpha =D_{\text{metric}}(\pi \vert \tilde{\pi})$ )이 솔루션이 된다. 해당 논문에서는 KL divergence를 확률 분포상의 거리 메트릭 기준으로 진행하였다. 이때의 가장 큰 문제점은 $\alpha$에 대한 constraints를 가지는 $L$의 최대화 수식(Objective)에서, 최적의 $\theta$값을 찾기 위해 Approximation을 진행하고, 이를 위해 <u>Hessian(2차 미분)을 수행해야한다는 점</u>이다. 이를 우회해서 풀 수 있는 방법으로 <u>Fisher Information Matrix</u>가 있는데, Gradient의 공분산을 Empirical하게 평균내면 Hessian에 근사할 수 있는 방법이다. TRPO에서는 $k=10$ 정도에서 타협을 했고, 이를 통해 Policy gradient의 안정적인 강화학습을 제안했던 논문이다.

PPO에서는 TRPO의 이런 타협점을 개선할 수 있는 방향을 제시한다. TRPO에서 constraint인 KL divergence에 적용되던 2차 미분(이를 surrogate objective로 표현한다)대신 1차 미분에 근사할 수 있는 방법을 찾고자 했다. PPO에서 적용한 방법은 기존의 surrogate objective에서 old/new policy가 크게 벗어나는 지점을 비율로 조절하게 된다($1-\epsilon$, $1+\epsilon$). 이러한 개념은 TRPO에서의 constraints의 $2^{nd}$ derivatives에서 $\pi \simeq \tilde{\pi}$ 인 지점을 찾고자 하는 주목적을 Hessian에서 단순 clipping으로 간소화시켰다고 이해할 수 있다.

사실 PPO/TRPO에 쓰이는 Q-function은 Advantage function $A$로 사용하는데, 이에 대한 보다 자세한 내용은 다음 섹터인 GRPO에서 언급하며 시작하도록 하겠다.

<p align="center">
    <img src="https://github.com/user-attachments/assets/df18835d-b0f2-4ab2-afed-49e58b4b0dd0" width="700">
</p>

### GRPO

이전에 올렸던 글 중 챗지피티는 어떤 식으로 학습되었을까에 대해 간단하게 소개했던 내용 중, RLHF와 PPO에 대해 간단하게 넘어갔던 부분이 있다. 딥시크는 PPO가 아닌 GRPO라는 자체적으로 연구한 방법을 통해 강화학습을 수행하였고, 강화학습을 통해 학습된 가장 고성능의 모델을 여러 작은 모델에 distillation하는 과정을 수행하였다. GRPO는 [“DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models”](https://arxiv.org/pdf/2402.03300)라는 논문에서 제안된 방법인 듯하다. 강화학습 논문이라기보다 <u>Deepseek에서 Language model의 수학적 추론 능력을 강화하기 위한 방법론</u>으로 제안하였다.

\[
\mathcal{J}\_{PPO}(\theta) = \mathbb{E} \left( q \sim P(Q), o \sim \pi\_{\theta_{\text{old}}} (O | q) \right) \frac{1}{|o|} \sum\_{t=1}^{|o|} \min \left( \frac{\pi\_{\theta} (o\_t | q, o\_{<t})}{\pi\_{\theta\_{\text{old}}} (o\_t | q, o\_{<t})} A\_t, \text{clip} \left( \frac{\pi\_{\theta} (o\_t | q, o\_{<t})}{\pi\_{\theta\_{\text{old}}} (o\_t | q, o\_{<t})}, 1 - \epsilon, 1 + \epsilon \right) A\_t \right).
\]

LLM에 위와 같은 PPO가 적용되며 다음과 같은 문제점이 생긴다. 우선 갑자기 등장한 수식에 맛있게 먹었던 점심이 다시 올라올 것 같기 때문에 천천히 보고 넘어가면 다음과 같다. 앞서 주구장창 언급했기 때문에 policy model에 대한 notation은 어느 정도 유추가 된다. 업데이트되기 전 파라미터가 $\theta_\text{old}$이고 업데이트된 파라미터는 $\theta$이다. $q, o$는 question dataset으로부터 샘플링된 질문, old policy로부터의 output을 의미한다(성능 비교의 베이스라인이 되는 LLM이 내뱉는 답변). 그리고 PPO에서의 clipping을 통해 학습 안정화하는 과정 또한 동일하다. 앞서 단순하게 넘어갔던 Advantage가 여기서 등장하는데, <u>기존의 Advantage function estimator들이 가지고 있는 문제점</u>(특히 state의 변화 갯수 $k$에 따른 bias/variance trade-off, $k$가  크면 variance가 커지고, $k$를 줄이게 되면 bias가 커지는 문제)를 보다 일반화하는 방법으로 소개된 Generalized Advantage Estimator(GAE)을 짚고자 한다. 우선 시작하기 전, estimator에 대한 정의부터 알아야한다.

\[
\begin{aligned}    \hat{A}\_t^{(1)} &= r\_t + \gamma V(s\_{t+1}) - V(s\_t) \newline    \hat{A}\_t^{(2)} &= r\_t + \gamma r\_{t+1} + \gamma^2 V(s\_{t+2}) - V(s\_t) \newline    &\quad \vdots = \vdots \newline    \hat{A}\_t^{(\infty)} &= r_t + \gamma r\_{t+1} + \gamma^2 r\_{t+2} + \cdots - V(s\_t)\end{aligned}
\]

현재의 policy가 앞으로의 state 변화에 끼칠 영향력을 생각한다. Value를 판단할 function $V$ 또한 학습 가능한 하나의 뉴럴 네트워크이다. 이렇듯 policy 모델과 함께 value 평가 모델이 함께 학습되는 구조를 <u>actor-critic model</u>이라 부른다.그런데 이때, time step+1인 시점과 time step+$k$인 시점의 가치는 서로 다르다. 당연하게도 만약 discounting value $\gamma$가 적용되지 않는다면 estimator가 보는 시점이 길어지면 길어질수록 그만큼 보다 가까운 다음 state에 대한 미래 가치를 상실하게 되고, 결국 <u>policy 및 value에 대한 공정 평가가 어렵기 때문</u>이다. 하지만 이러한 Advantage Function에 문제가 있다.

작은 $k$값을 갖는 추정량 $A_t^{(k)}$는 분산이 낮지만 편향이 크고, 큰 $k$값을 갖는 경우 편향이 낮지만 분산이 크다는 점이다. 이는 <u>항의 개수를 보면 직관화가 가능</u>하다. 합산해야 할 항이 적을 경우(state의 변화가 많지 않기 때문에) 분산이 낮아지지만, 상대적으로 $r_k$에 대한 정확한 정보를 활용하지 않기 때문에 편향이 상대적으로 커진다. 합산해야 할 양이 큰 경우는 반대로 생각하면 된다. 또한, $V(s_t)$ 가 추정 클래스 내에서는 상수로 생각될 수 있기 때문에, 추정량 간 차이는 오직  $k-step$ return에서만 발생하게 된다(길게 표현했지만 trade-off가 존재한다는 사실).

그렇기 때문에 GAE는 <u>추정량 집합 전체에 대해 싸그리 어셈블하는 전략을 사용</u>하였다. 각 estimator에 존재하는 trade-off를 <u>$\lambda$로 조절하겠다는 생각</u>이다.

\[
\begin{aligned}    \hat{A}\_t^{GAE(\gamma, \lambda)}    &= (1 - \lambda) \left( \hat{A}\_t^{(1)} + \lambda \hat{A}\_t^{(2)} + \lambda^2 \hat{A}\_t^{(3)} + \cdots \right) \newline    &= (1 - \lambda) \left( \delta\_t^V + \lambda (\delta\_t^V + \gamma \delta\_{t+1}^V) + \lambda^2 (\delta\_t^V + \gamma \delta\_{t+1}^V + \gamma^2 \delta\_{t+2}^V) + \cdots \right) \newline    &= (1 - \lambda) \left( \delta\_t^V (1 + \lambda + \lambda^2 + \cdots) + \gamma \delta\_{t+1}^V (\lambda + \lambda^2 + \cdots) + \cdots \right) \newline    &= (1 - \lambda) \left( \delta_t^V \frac{1}{1 - \lambda} + \gamma \delta\_{t+1}^V \frac{\lambda}{1 - \lambda} + \cdots \right) \newline    &= \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta\_{t+l}^V\end{aligned}
\]

바로 이 개념이 GAE이고, PPO 또한 이를 적용한 모델이라고 볼 수 있다. 여기서의 문제점을 드러내면 다음과 같다. Value model은 Policy model이 학습됨에 따라 내뱉는 <u>output에 대한 성능 평가를 진행</u>하는데, 이때 Policy model과 별개로 학습이 진행되어있어야한다는 점이다.  PPO에서는 reward model에 과적합되는 문제를 직접적으로 피하기 위해 <u>KL penalty</u>를 도입하게 된다. Reference model은 주로 초기 언어 모델이나 SFT(Supervised Fine-tuned) 모델로 사용하게 되는데, 학습되는 정책 모델이 내뱉는 output이 SFT 모델과 지나치게 달라지지 않도록 각 토큰 단위에서 제약을 가하게 되어 생성되는 텍스트의 <u>각 토큰이 기준 모델에서 기대되는 분포에서 크게 벗어나지 않도록</u> 한다. 

\[
r_t = r_{\varphi} (q, o_{\leq t}) - \beta \log \frac{\pi_{\theta} (o_t | q, o_{<t})}{\pi_{\text{ref}} (o_t | q, o_{<t})}
\]

<p align="center">
    <img src="https://github.com/user-attachments/assets/561b5870-65d3-4abe-8dbd-9513819b1c77" width="800"/>
</p>

그런데 실제 GRPO 논문의 그림을 보면 살짝 이해하기 어려운 부분이 있을텐데, 바로 KL divergence의 방향성이다. 그런데 실질적으로 KL penalty가 적용되는 구조는 동일하다. PPO 기반으로 동작하는 것은 거의 유사하지만, 바뀐 점은 다음과 같다.

<p align="center">
    <img src="https://github.com/user-attachments/assets/4dc7d92e-01ad-41bb-a512-a03062eb56f9" width="800"/>
</p>

- PPO와 달리 GRPO는 더이상 Value Model을 사용하지 않는다. 그렇기 때문에 메모리를 절약할 수 있다.
- 그러나 Value Model을 사용할 수 없다는 것은 GAE를 사용할 수 없다는 뜻이고, 결국 기존에 연산되던 advantage function $A$를 새롭게 정의해야한다.
- 두 정책 모델의 TR 내에서 강화학습을 진행하는 구조는 동일하다. 그러나 이번에는 동일 질문($q$)에 대해 여러 output $o_{1 \ldots G}$를 답변으로 추출한다.
- 그리고 이 각각의 답변에 대해 예측된 이득의 평균을 정책 강화 학습의 Objective로 사용할 것이다.
- 학습된 reward 모델로부터 각각의 output을 평가한다.
- Reward model의 결과 $\{r_1, r_2, \cdots, r_G\}$를 그룹 전체의 reward로 normalize해서 사용한다. 이는 특정 문맥 $q/o$에 대한 bias/variance(GAE에서 문제가 된 부분)를 줄이기 위한 노력이다.
- KL divergence는 기존의 식이 아닌 Unbiased Estimator를 사용하였다.

요약하자면, Value function을 없앴기 때문에 <u>불가능한 GAE 대신 Reward의 grouped output을 정규화</u>하여 trade-off 효과를 상쇄하였으며, KL divergence 식이 바뀐 정도가 되겠다.

\[
\mathcal{J}\_{GRPO}(\theta) = \mathbb{E} \left( q \sim P(Q), \{o\_i\}\_{i=1}^{G} \sim \pi\_{\theta\_{\text{old}}} (O | q) \right) \\
\frac{1}{G} \sum\_{i=1}^{G} \left( \min \left( \frac{\pi\_{\theta} (o\_i | q)}{\pi\_{\theta\_{\text{old}}} (o\_i | q)} A\_i,
\text{clip} \left( \frac{\pi\_{\theta} (o\_i | q)}{\pi\_{\theta\_{\text{old}}} (o\_i | q)}, 1 - \epsilon, 1 + \epsilon \right) A\_i \right) - \beta \mathbb{D}\_{KL} (\pi{\theta} \| \pi\_{\text{ref}}) \right)
\]

여기서 쓰인 KL divergence 식은:

\[
\mathbb{D}\_{KL} (\pi\_{\theta} \| \pi\_{\text{ref}}) = 
    \frac{\pi\_{\text{ref}} (o_i | q)}{\pi\_{\theta} (o\_i | q)} - \log \frac{\pi\_{\text{ref}} (o\_i | q)}{\pi\_{\theta} (o\_i | q)} - 1.
\]

이며 각각의 리워드는 다음과 같이 정규화하여 연산된다:

\[
A_i = \frac{r_i - \operatorname{mean} (\{r_1, r_2, \dots, r_G\})}{\operatorname{std} (\{r_1, r_2, \dots, r_G\})}.
\]

# Methods / Approaches

### Deepseek-R1 Zero의 학습법

딥시크는 <u>DeepSeek-V3-Base를 베이스 모델</u>로 활용, <u>GRPO를 가장 메인인 강화 학습법</u>으로 적용한 연구이다. 그 중 가장 메인이 되는 학습법에는 크게 리워드 모델링과 학습 템플릿이 있다. 이외의 방법론은 직접 학습을 통해 demonstration 과정을 거쳤다. 딥시크의 가장 큰 주장은, LLM 베이스 모델을 강화학습 만으로도 o1 만큼의 코딩/수학적 능력까지 키울 수 있다는 사실로, 강화학습이 LLM 학습에 큰 도움이 된다는 사실을 입증한다. Deepseek-R1은 Ollama에서 경량화 버전을 바로 사용해볼 수 있는데, 이때의 output을 확인해보면 실제 답변을 하기 전 ‘생각하는 부분’이 추가된 것을 알 수 있다. 개인적으로 맥북 프로 (M3)를 사용중인데, 속도 면에서 deepseek-r1:14b 모델 정도의 distillation 버전 정도면 편하게 돌릴 수 있는 것 같다. Deepseek-R1:14B 버전에게 LLM system에 대해 질문한 예시는 다음과 같다. 물론 이렇게 사용한 모델은 R1-Zero는 아니다. 그냥 예시를 보여주기 위해 사용하였다.

<p align="center">
    <img src="https://github.com/user-attachments/assets/55ed6a24-a3a4-44b3-b66b-3bfa9adf744e" width="962"/>
</p>

딥시크는 보는 바와 같이 추론 과정을 \<think\> ~ \</think\> 태그로 넣고, 그 이후에 실제 답변을 내놓는 것을 볼 수 있다(근데 distill 버전을 쓰다보면 가끔 생각을 멈추기도 함). 이러한 답변 특성은 <u>학습 템플릿 구성</u>에 있다.

<p align="center">
    <img src="https://github.com/user-attachments/assets/e27bbdec-90c2-4f8d-8a37-484ba4a9475a" width="962"/>
</p>

결국 o1이고 o3고 어떤 방식으로 추론해서 정답을 내놓는지는 모르겠지만, 이 <u>일련의 과정을 강화학습으로 자동 학습하게 하는 것이 주된 아이디어인 듯</u>하다. 이에 따라 R1-Zero의 Reward modeling 또한 두 분류로 나뉘게 되었다. 리워드 모델을 따로 학습하여 사용할 수도 있지만, Deepseek-R1-Zero 경우 Rule-based method (blackbox가 아닌 평가가 예측 가능한 경우 사용)를 적용하였다. 주요 보상 유형은 두 가지로 구성된다. 개발하는 과정에서 결과 기반 또는 과정 기반 신경망 보상 모델은 적용하지 않았다. 

- **정확성 보상:** 정확성 보상 모델은 응답이 올바른지 평가한다. 예를 들어, 결정적인 결과를 가진 수학 문제의 경우, 모델이 특정한 형식으로 정답을 제시해야 하며, 이를 통해 신뢰할 수 있는 규칙 기반 검증이 가능하다. 마찬가지로, LeetCode (코딩) 문제의 경우, 사전에 정의된 테스트 케이스를 기반으로 컴파일러를 활용하여 피드백을 생성할 수 있다.
- **형식 보상:** 정확성 보상 모델 외에도, 형식 보상 모델을 적용하여 모델이 사고 과정을 <think> 및 </think> 태그 사이에 넣도록 강제한다.

### 흥미로운 점

학습 도중 발견한 흥미로운 내용으로 <u>‘아하 모먼트’</u>를 언급한다. 처음엔 잘못 본 줄 알았는데 정말 아하 모먼트였다. 학습 중간의 정책 모델에서 아하 모멘트가 등장하고 난 후, 사고하는 시간에 보다 시간을 할애하고(스스로 생성한 think가 보다 풍부해진다는 뜻 같음) 본인의 초반 접근법(아하 포인트 이전의 의식의 흐름)을 자체 평가하고 개선하는 과정이 추가되었다는 것이다 (아래 예시).

<p align="center">
    <img src="https://github.com/user-attachments/assets/8072eb65-cea5-4f23-b29f-705319ba7e44" width="400">
    <img src="https://github.com/user-attachments/assets/90323441-3efb-4410-9066-d47f60f1a13a" width="400">
</p>

답변을 내놓는 LLM은 하나의 의사 결정을 하는 정책 모델이다. 이 정책 모델을 학습하기 위해 기준선인 reward를 세우고 템플릿에 맞춰 자체적으로 발전할 수 있게 했더니 o1의 성능을 넘었다는 연구가 되었다.

### Deepseek-R1의 학습법

흥미롭지만 일단 여기까지는 <u>수학문제나 코딩에 대한 부분</u>이다. LLM은 보다 다양한 task에 적용되어야하는데, 다음 step으로 어떤 방법을 썼을까?
Deepseek-R1-Zero로부터 알아낸 결과는 템플릿과 강화 학습의 조합으로 충분히 좋은 추론 능력을 키울 수 있다는 점이었다. 그렇다면 다음과 같은 의문이 들게 된다.

1. 강화학습 과정은 초반 학습이 불안정하다는 단점이 있다. 이때 만약 적은 양의 high-quality data(사전 학습 데이터)를 통해 수렴 속도를 줄일 수 있는가? (R1-Zero는 오로지 강화학습만으로 학습함)
2. 수학이랑 코딩 능력 말고 일반화 능력이 강화됨과 동시에, 앞서 했던 것과 같이 CoT 과정이 보다 사용자 친화적이면서 명확해질 수 있는 방법이 있는가? (R1-Zero는 수학 문제랑 코딩만 함)

이를 해소하기 위해 Cold Start 방식을 사용하였다. RL을 통해 DeepSeek-V3-Base 모델을 처음부터 학습하는 것이 아닌, 어느 정도 길이의 CoT를 포함한 question/answer 쌍을 답변 데이터로 구성하고 이를 사용하여 fine-tuning한 모델을 시작 포인트로 삼는다. 해당 데이터는 기존 모델의 성능을 해치지 않고 사후의 RL 학습에 도움이 되어야하기 때문에 <u>최대한 깔끔하게 curating하는 과정</u>을 거치며, 모집한 샘플은 대략 $8\times 10^5$개다.

그러나 Cold Start로 미세 조정한 모델을 RL로 학습했을때 language mixing 문제가 발생하였는데, 이는 추론 과정이나 답변 과정에 하나의 언어만 포함되지 않고 여러 언어가 포함되는 경우(한글, 중국어, 영어 혼용 등등) 가 발생한다는 사실이다.

실제로 DeepSeek 사용자들의 리뷰를 봤을 때 <u>한국어를 사용할 경우 종종 경량화 모델에서 언어가 섞여서 출몰한다는 경험담</u>이 있었는데, 아마 이 문제를 완벽하게 해결하지는 못했나보다. 아무튼 이를 그나마 좀 해소하기 위해 선택한 방법은 RL training 과정에 추가 reward로 language consistency를 준다는 것이다. 해당 리워드 때문에 성능이 좀 저하는 되지만 그래도 일단 LLM이라면 알아듣게는 말하는게 선호도가 더 높다는 것이 DeepSeek의 판단.

### Distilled Model

Distillation 과정을 통해 원본 모델에 비해 상대적으로 경량화된 버전을 내놓았다.

<p align="center">
    <img src="https://github.com/user-attachments/assets/77e15c84-fec2-4d0e-ae44-daf3739da40d" width="700"/>
</p>

앞서 Cold Start 때문에 모아놨던 샘플을 똑같이 Llama, Qwen 미세 조정에 적용하고, 이후에는 RL 학습을 하지 않고 SFT 방식을 적용했다. 이유는 <u>아래와 같이 RL로 학습하는 것보다, R1 모델로부터 증여받는 것이 훨씬 이득이었기 때문</u>이다. **이래서 금수저 집안이 좋은가보다ㅠㅠ**. 그렇다해서 RL 성능이 너무 낮은 것은 또 아니다.

<p align="center">
    <img src="https://github.com/user-attachments/assets/b7d5ee81-2b05-4a48-b802-c5dc0ed455af" width="700"/>
</p>

근데 사실 LLM 써보면 느끼는 점인데 추론 문제에 대한 정량평가는 수치일 뿐 실제 사용했을때 유저 평가 지표가 좀 더 중요할 듯 하다.

# 내 생각
DeepSeek는 오픈소스이다. OpenAI는 클로즈소스이다. 이게 가장 큰 장점이라고 할 순 있겠다. 그런데 메모리 사양이 딸려서 원본 모델을 직접 올려볼 수는 없지만 정말 필요하다면 서버를 확보하고 사용할 수는 있을 것 같다.
